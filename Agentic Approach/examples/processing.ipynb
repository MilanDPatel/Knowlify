{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c75f68e2",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f9ecaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx in /opt/anaconda3/lib/python3.12/site-packages (0.27.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92efe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIz...M\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "import httpx\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types as gemini_types\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from eduly import EdulyAnimationClient, EdulyBreakdownClient\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "aistudio_gemini_api_key = os.environ['GOOGLE_API_KEY']\n",
    "print(aistudio_gemini_api_key[:3] + '...' + aistudio_gemini_api_key[-1:])\n",
    "gemini_client = genai.Client(api_key=aistudio_gemini_api_key)\n",
    "\n",
    "MODEL_NAME = \"gemini-3-pro-preview\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b21b1",
   "metadata": {},
   "source": [
    "# Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e6151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eduly_breakdown_client = EdulyBreakdownClient(gemini_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4cde504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-3-pro-preview\n",
      "models/gemini-3-flash-preview\n",
      "models/gemini-3-pro-image-preview\n",
      "models/nano-banana-pro-preview\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n",
      "models/deep-research-pro-preview-12-2025\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "models/aqa\n",
      "models/imagen-4.0-generate-preview-06-06\n",
      "models/imagen-4.0-ultra-generate-preview-06-06\n",
      "models/imagen-4.0-generate-001\n",
      "models/imagen-4.0-ultra-generate-001\n",
      "models/imagen-4.0-fast-generate-001\n",
      "models/veo-2.0-generate-001\n",
      "models/veo-3.0-generate-001\n",
      "models/veo-3.0-fast-generate-001\n",
      "models/veo-3.1-generate-preview\n",
      "models/veo-3.1-fast-generate-preview\n",
      "models/gemini-2.5-flash-native-audio-latest\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025\n",
      "models/gemini-2.5-flash-native-audio-preview-12-2025\n"
     ]
    }
   ],
   "source": [
    "for m in gemini_client.models.list():\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25699f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "breakdown_obj, raw_breakdown_response = eduly_breakdown_client.breakdown(\n",
    "    file_path=pathlib.Path(\"./rlmpaper.pdf\"),\n",
    "    model=MODEL_NAME,\n",
    "    thinking_level=\"high\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3940646f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakdown saved successfully!\n"
     ]
    }
   ],
   "source": [
    "file_path = pathlib.Path('./cached_outputs/rlm_breakdown.pkl')\n",
    "\n",
    "# Make sure the folder exists\n",
    "file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the breakdown object\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(breakdown_obj, f)\n",
    "\n",
    "print(\"Breakdown saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b42d4717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12183"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.getsize('./cached_outputs/rlm_breakdown.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f75a1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./cached_outputs/rlm_breakdown.pkl', 'rb') as f:\n",
    "    breakdown_obj = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df745627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'recursive_language_models'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "santised_title = breakdown_obj.document_title.replace(\" \", \"_\").lower()\n",
    "santised_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3160501a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: The Problem of Context Rot in Long-Context LLMs\n",
      "Topic 1: Recursive Language Models (RLMs): The Core Architecture\n",
      "Topic 2: Task Complexity and Information Density\n",
      "Topic 3: Emergent Behaviors: How RLMs Actually Reason\n",
      "Topic 4: Performance Results and Cost Analysis\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(breakdown_obj.topics):\n",
    "    print(f\"Topic {i}: {topic.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1db9b47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [03:32, 42.53s/it]\n"
     ]
    }
   ],
   "source": [
    "storyboards = {}\n",
    "\n",
    "for i, topic in tqdm(enumerate(breakdown_obj.topics)):\n",
    "    storyboard_obj, raw_storyboard_response = eduly_breakdown_client.storyboard(\n",
    "        topic=topic,\n",
    "        model=MODEL_NAME,\n",
    "        thinking_level=\"high\",\n",
    "        source_file=\"./rlmpaper.pdf\"\n",
    "    )\n",
    "\n",
    "    storyboards[topic.name] = storyboard_obj\n",
    "\n",
    "    with open(f'./cached_outputs/{santised_title}_storyboard_{i}.pkl', 'wb') as f:\n",
    "        pickle.dump(storyboard_obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46d5381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "storyboards = {}\n",
    "\n",
    "for i, topic in enumerate(breakdown_obj.topics):\n",
    "    with open(f'./cached_outputs/{santised_title}_storyboard_{i}.pkl', 'rb') as f:\n",
    "        storyboards[topic.name] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a0d5e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The Problem of Context Rot in Long-Context LLMs': TopicStoryboard(topic_name='The Problem of Context Rot in Long-Context LLMs', visual_concept=\"The central visual metaphor is 'The Overloaded Workbench.' We contrast a model trying to memorize an entire library at once (context rot) versus a model sitting at a desk, fetching only the specific books it needs when it needs them (inference-time scaling/out-of-core algorithms).\", scenes=[Scene(scene_type='hook', title='The Infinite Scroll', visual_description=\"A sleek, glowing digital brain (representing an LLM) sits in the center. A stream of documents, books, and code enters its 'mind' from the left. A counter above the head counts up rapidly: '10,000 tokens', '100,000 tokens', '1,000,000 tokens'. The brain glows brighter, looking powerful. The background is a clean, dark void with grid lines.\", narration='In the world of artificial intelligence, we are witnessing an explosion of memory. Modern language models can now technically ingest entire libraries—millions of words—in a single prompt. It sounds like the ultimate super-power: perfect recall at an infinite scale.'), Scene(scene_type='mid', title='The Fog of Data', visual_description=\"The camera zooms into the brain. Inside, we see a representation of 'Attention'—lines connecting different words. At first, with few tokens, the lines are sharp and bright gold. As the token counter continues to rise to 1 million, the lines become a chaotic, tangled mess. The bright gold connections fade into a murky, grey fog. A red warning label appears: 'CONTEXT ROT'.\", narration=\"But there is a catch. Just because a model can *swallow* a million words, doesn't mean it can *digest* them. As the input grows, a phenomenon called 'Context Rot' sets in. The model's attention gets diluted, drowning in the noise of its own memory.\"), Scene(scene_type='mid', title='Needles vs. Haystacks', visual_description=\"Split screen. \\nLeft side: 'Simple Task'. A massive pile of hay. One glowing red needle. A spotlight scans the hay and instantly finds the needle. A checkmark appears.\\nRight side: 'Dense Task'. The same hay pile, but now there are hundreds of red threads woven together intricately. The spotlight tries to follow the pattern but gets lost, flickering and dimming. An 'X' appears.\", narration=\"It's not that the models simply forget. They can easily find a specific password hidden in a book—that's the 'needle in a haystack' problem. But ask them to track a complex plot or connect an idea from page one to page five hundred? That's where they crumble.\"), Scene(scene_type='mid', title='The Performance Cliff', visual_description=\"A 2D line graph appears. \\nX-axis: 'Input Length (Tokens)' (Log scale: 8k, 128k, 1M).\\nY-axis: 'Reasoning Accuracy'.\\nTwo lines are drawn. \\nLine 1 (Green, labeled 'Retrieval'): Stays flat and high.\\nLine 2 (Red, labeled 'Deep Reasoning'): Starts high, but plummets aggressively as it passes 100k tokens, crashing near zero.\", narration=\"This degradation isn't subtle. On tasks that require reasoning across the whole text, performance doesn't just dip—it dives off a cliff. The model's *physical* context window is huge, but its *effective* reasoning window is surprisingly small.\"), Scene(scene_type='mid', title='The Memory Bottleneck', visual_description=\"We transition to a computer hardware analogy. \\nOn the left: A small, fast microchip labeled 'RAM' (Context Window). \\nOn the right: A massive stack of hard drives labeled 'Environment' (The Prompt).\\nAn animation shows a user trying to shove the entire hard drive stack into the tiny RAM chip. The chip smokes and turns red.\", narration=\"Think of it like a computer. Trying to stuff a million-token prompt into the model's active attention is like trying to load a terabyte database entirely into your RAM. It's inefficient, expensive, and frankly, overwhelming.\"), Scene(scene_type='mid', title='Out-of-Core Inspiration', visual_description=\"The smoking chip is replaced by a calm, rhythmic system. \\nData flows from the 'Hard Drive' (Prompt) to the 'RAM' (Model) in small, neat packets. \\nThe Model processes a packet, writes a note, discards the packet, and requests the next one. \\nText appears: 'Out-of-Core Algorithm'.\", narration=\"Computer science solved this decades ago with 'out-of-core' algorithms. If a dataset is too big for memory, you don't crash. You leave the data on the disk, and you fetch only the chunks you need, when you need them.\"), Scene(scene_type='mid', title='Inference-Time Scaling', visual_description=\"Now we apply this to the LLM. \\nThe 'Prompt' is visualized as a long scroll sitting on a desk (the 'Environment'). \\nThe LLM appears as a robotic hand holding a pen (a Python Interpreter). \\nThe hand writes code: `read(chapter_1)`, processes it, then writes `read(chapter_5)`. \\nThe massive scroll never enters the robot's head all at once.\", narration=\"This is the core idea behind 'Inference-Time Scaling.' Instead of feeding the prompt *into* the neural network, we treat the prompt as an external environment. The model uses code to browse the text, read specific snippets, and reason iteratively.\"), Scene(scene_type='closing', title='The Flat Line of Success', visual_description='We return to the graph from Scene 4. \\nThe red crashing line (Base Model) is still there. \\nA new Blue Line (Recursive Model) animates. It follows the red line initially, but where the red line crashes, the blue line stays perfectly horizontal—maintaining high accuracy even at 1 million tokens. \\nConfetti or sparkles erupt along the high plateau of the blue line.', narration=\"The result? We stop the rot. By processing information in manageable pieces, we can extend the reasoning capabilities of models by orders of magnitude. It turns out the secret to reading a long book isn't a bigger brain—it's learning how to turn the pages.\")]),\n",
       " 'Recursive Language Models (RLMs): The Core Architecture': TopicStoryboard(topic_name='Recursive Language Models (RLMs): The Core Architecture', visual_concept=\"We visualize the RLM as a 'Model-in-the-Loop' system. The central metaphor is a 'programmer' (the Root LM) sitting at a terminal (the REPL) separated from a massive library (the Prompt) by a glass wall. Instead of trying to stuff the library into its head, the programmer types commands to retrieve specific pages or dispatch assistants (Sub-LMs) to summarize sections, building the answer piece by piece.\", scenes=[Scene(scene_type='hook', title='The Context Bottleneck', visual_description=\"A standard Transformer architecture is depicted on the left as a complex network of nodes. On the right, a massive scroll of text (representing a book or codebase) attempts to feed into the model's input. As the scroll gets longer, it hits a glowing red barrier labeled 'Context Window Limit'. The excess text piles up and burns away, symbolizing lost information. A counter shows 'Tokens: 200k... 500k... 10M', and the model flashes 'Out of Memory'.\", narration='Imagine trying to memorize an entire library in a single glance. Even the most powerful AI models today face a hard limit: the context window. Feed them too much text, and they either crash or simply forget the beginning by the time they reach the end. We need a fundamental shift in how we handle massive inputs.'), Scene(scene_type='mid', title='Flipping the Perspective', visual_description=\"The scene wipes clean. The massive text scroll reappears, but now it is placed inside a distinct, box-like container labeled 'Environment (Python REPL)'. The Language Model (Root LM) sits *outside* this box. There is no direct pipe feeding the text into the LM. Instead, a thin line connects the LM to the Environment box, labeled 'Code Interface'.\", narration='Recursive Language Models, or RLMs, solve this by flipping the script. Instead of feeding the prompt directly into the neural network, the RLM treats the text as part of an external environment. Think of it less like reading a book, and more like managing a database.'), Scene(scene_type='mid', title='The Variable Abstraction', visual_description=\"Inside the 'Environment' box, the massive text scroll condenses into a single block labeled with the variable name `prompt`. A metadata bubble floats up to the Root LM: 'Variable: prompt, Type: String, Length: 10,000,000'. The Root LM displays a thought bubble containing Python syntax.\", narration=\"When an RLM starts, it doesn't see the text. It sees a variable. It knows it has a string called 'prompt' and it knows that string is ten million characters long. But the content remains inside the Python environment, safe and accessible, without clogging the model's brain.\"), Scene(scene_type='mid', title=\"The Programmer's Interface\", visual_description=\"The Root LM emits a line of code: `print(prompt[:200])`. This code travels down the line into the Environment. The REPL executes it. A small snippet of text ('Chapter 1: The beginning...') pops out of the `prompt` block and travels back up to the Root LM. The LM stores this small observation in its local context.\", narration='The model acts as a programmer. If it wants to know how the story starts, it writes a line of code to slice the string. The environment executes the code and returns just that specific snippet. The model can peek, probe, and navigate the data efficiently.'), Scene(scene_type='mid', title='The Recursive Call', visual_description='The Root LM writes more complex code: `chunk = prompt[5000:6000]` followed by `summary = llm_query(chunk)`. Inside the Environment box, a smaller, semi-transparent version of the neural network appears (the Sub-LM). It consumes the `chunk`, processes it, and turns it into a small summary node. This summary is returned to the Root LM.', narration='But here is the true superpower: recursion. The model can write code that calls *itself*. It can carve out a specific chapter and pass it to a sub-instance of the model, asking it to summarize or analyze just that piece. The sub-model does the heavy lifting and returns a concise result.'), Scene(scene_type='mid', title='The Loop (Read-Eval-Print)', visual_description=\"A cyclic animation begins. 1. Root LM generates code. 2. Environment executes code (sometimes spawning Sub-LMs). 3. Output returns to Root LM. 4. Root LM updates its state. The cycle repeats rapidly. As it loops, a 'Final Answer' document constructs itself on the side, line by line, derived from these interactions.\", narration=\"This creates a loop. Read, Evaluate, Print. The model programs its own investigation, delegating tasks to sub-models, aggregating their findings, and deciding what to check next. It's not just reading; it's actively reasoning over the data.\"), Scene(scene_type='closing', title='Infinite Scaling', visual_description=\"We zoom out to show the `prompt` variable growing larger and larger—100 million tokens, 1 billion tokens. The Root LM remains the same size, calm and unburdened. The 'Code Interface' line pulses steadily, handling the massive scale with ease. The background transitions to a vast library being managed by a single, efficient librarian.\", narration=\"By decoupling the input size from the model's architecture, RLMs theoretically unlock infinite context. The limit is no longer how much the model can think about at once, but simply how much memory the computer environment can hold. It turns the passive reader into an active explorer.\")]),\n",
       " 'Task Complexity and Information Density': TopicStoryboard(topic_name='Task Complexity and Information Density', visual_concept=\"We visualize the input text as a stream of data blocks. We classify tasks by how 'dense' the connections between these blocks are. 'Constant' complexity is finding one glowing block. 'Linear' complexity is processing every block individually (like an assembly line). 'Quadratic' complexity is drawing a web of connecting lines between every possible pair of blocks, illustrating why standard models choke on the computational load while RLMs handle it systematically.\", scenes=[Scene(scene_type='hook', title='The Context Rot Cliff', visual_description=\"A 2D graph appears. The X-axis is 'Context Length' (log scale, 8k to 1M tokens). The Y-axis is 'Performance'. Three lines appear. \\n1. Top line (Blue): A flat, high line labeled 'Finding a Needle'. \\n2. Middle line (Orange): Starts high, slowly dips. Labeled 'Summarizing'.\\n3. Bottom line (Red): Starts high, then plummets to near zero rapidly. Labeled 'Pairwise Reasoning'.\\n\\nA spotlight highlights the 'Red Zone' where the Red line crashes. The text 'Context Rot' fades in over the drop.\", narration=\"We often ask: how much text can an AI read? But that's the wrong question. The real question is: what do you want it to *do* with that text? Not all long documents are created equal. Depending on the task, a model's brain might break down at ten thousand words, or sail smoothly past a million.\"), Scene(scene_type='mid', title='Defining Information Density', visual_description=\"The screen clears. A long horizontal stream of gray data blocks moves from right to left, representing a long prompt. A magnifying glass appears. \\nThree icons appear below the stream:\\n1. A single Diamond (S-NIAH)\\n2. A row of Bricks (OOLONG)\\n3. A complex Spiderweb (OOLONG-Pairs)\\n\\nAn arrow points from 'Input Size' to 'Processing Cost', changing thickness based on the icon selected.\", narration='The paper introduces the concept of Information Density. It categorizes tasks by how hard they are to compute as the document gets longer. We call these Constant, Linear, and Quadratic complexity.'), Scene(scene_type='mid', title='Level 1: The Needle (Constant)', visual_description=\"Focus on the Diamond icon. The stream of gray blocks pauses. Somewhere deep in the middle, one single block glows bright cyan. \\nA standard 'Attention Spotlight' scans frantically back and forth.\\nThen, the view splits. On the right, a Python terminal appears. It types `text.find('secret_code')`. It instantly snaps to the cyan block without scanning the rest.\", narration=\"First, the simplest level: Needle in a Haystack. You're looking for one specific phrase, like a password. Whether the book is ten pages or a thousand, the answer is just one tiny piece. Standard models are okay at this, but a Recursive Language Model is smarter—it doesn't read; it uses code, like a search function, to jump straight to the answer.\"), Scene(scene_type='mid', title='Level 2: The Assembly Line (Linear)', visual_description=\"Focus on the Brick row icon. The stream of gray blocks transforms so that *every* block has a unique number on it (10, 5, 30, 2...). \\nTask text appears: 'Sum all numbers'.\\n\\nMethod A (Summarization): A giant trash compactor labeled 'Summarizer' crushes the blocks into a small, blurry cube. The numbers are unreadable. Result: 'Approx 50?'.\\n\\nMethod B (RLM): The blocks move along a conveyor belt. A robotic arm (The Sub-LM) picks up one block, reads it, adds it to a running total on a scoreboard, and tosses it. It processes 100% of the blocks one by one.\", narration='Next is Linear complexity. Here, you need to process every single sentence to get the answer—like summing numbers in a ledger. Summarization agents fail here because summarizing compresses the data, blurring out the specific details you need. But an RLM treats this like a for-loop. It processes chunk by chunk, aggregating the data without losing a single digit.'), Scene(scene_type='mid', title='Level 3: The Web (Quadratic)', visual_description=\"Focus on the Spiderweb icon. The stream of blocks appears again. \\nTask text: 'How does every character relate to every other character?'.\\n\\nLines begin to draw from the first block to *every other block* in the sequence. Then from the second block to every other block. \\nThe screen rapidly fills with a dense, messy mesh of lines. \\nA standard AI model icon (a brain) appears, overheats, and turns red (representing the crash in Figure 1).\", narration=\"This is the boss level: Quadratic complexity. The 'Pairwise' task. Here, you aren't just reading chunks; you have to compare every chunk to every other chunk. If you double the document length, the work doesn't just double—it quadruples. Frontier models like GPT-5 crash completely here. Their attention mechanisms just can't hold that many connections at once.\"), Scene(scene_type='mid', title=\"RLM's Quadratic Solution\", visual_description=\"The messy web clears. We see the Python terminal again. \\nCode appears: \\n`for i in chunks:`\\n`  for j in chunks:`\\n`    compare(i, j)`\\n\\nThe visualization shows two pointers. The 'i' pointer stays on Block 1. The 'j' pointer slides across all other blocks, drawing a clean, single line for each comparison. Then 'i' moves to Block 2, and 'j' scans again. It is slow, methodical, and organized.\", narration=\"This is where the Recursive Language Model proves its worth. It doesn't try to hold the whole web in its head. It writes a nested loop. It compares piece A to piece B, then piece A to piece C, systematically working through the pairs. It takes more time, but unlike the standard model which scores nearly zero, the RLM actually solves the problem.\"), Scene(scene_type='closing', title='The Right Tool for the Job', visual_description=\"Split screen into three horizontal bands.\\nTop: A search bar finding a needle (Constant).\\nMiddle: A conveyor belt processing items (Linear).\\nBottom: A matrix grid filling up cell by cell (Quadratic).\\n\\nText overlay: 'RLM adapts the strategy to the density.'\", narration=\"The key insight is that 'Long Context' isn't just one problem. It's a spectrum of density. By offloading the memory management to code—loops, variables, and searches—Recursive Language Models adapt their strategy to match the complexity of the task, succeeding where biological-style attention fails.\")]),\n",
       " 'Emergent Behaviors: How RLMs Actually Reason': TopicStoryboard(topic_name='Emergent Behaviors: How RLMs Actually Reason', visual_concept=\"The RLM is depicted as a 'Master Architect' sitting at a workbench (the REPL). Instead of trying to memorize a massive blueprint (the prompt) all at once, the Architect uses specific tools—a magnifying glass (Regex), a pair of scissors (Chunking), and a checklist (Verification)—to manipulate the blueprint externally. The code blocks appear as glowing blue instructions that animate the tools.\", scenes=[Scene(scene_type='hook', title='The Impossible Memory Test', visual_description='A massive, towering stack of papers (representing 10 million tokens) looms over a small, standard brain icon. The brain tries to absorb it all, glowing red and shaking, indicating overload. Suddenly, the stack is pushed aside. An identical brain appears, but this one is sitting at a desk with a computer terminal. It looks calm. On the terminal screen, a single line of code appears: \\'search(\"date\", document)\\'. The massive stack instantly vanishes, leaving only a single sheet of paper floating in the air.', narration=\"Imagine asking a standard AI to memorize a library's worth of text just to find one specific date. It would likely crash or hallucinate. But Recursive Language Models don't try to memorize. They do something much more human: they cheat. Well, they use tools.\"), Scene(scene_type='mid', title='The Environment as a Workspace', visual_description=\"The screen splits. On the left is the 'Model' (a glowing node network). On the right is the 'Environment' (a dark terminal window). A long scroll labeled 'PROMPT_VAR' sits inside the terminal, NOT inside the Model. The Model shoots a beam of light (code) into the terminal. The code reads: `len(PROMPT_VAR)`. The terminal returns `10,000,000`. The Model nods. It never pulls the full scroll into itself.\", narration=\"The key is that the long text isn't fed directly into the model's brain. It sits in a Python environment as a variable. The model just looks at the variable name, realizes it's too big to read, and decides to write code to peek at it instead.\"), Scene(scene_type='mid', title='Emergent Behavior 1: The Regex Searchlight', visual_description=\"We zoom into the terminal. The text variable is a long, blurry gray block. The Model generates code: `re.search(r'Festival.*La Union', text)`. As this code executes, a bright yellow 'searchlight' beam scans across the gray text block. It ignores 99% of the text, moving rapidly. It stops and illuminates just two lines: '...annual arts festival held in La Union...' The rest of the text fades to black. The Model absorbs only these two lines.\", narration=\"The first surprise behavior is filtering. Without being explicitly trained to do so, these models started writing Regular Expressions. Like a researcher using 'Control-F', they use keyword searches to pinpoint exactly what they need, ignoring millions of irrelevant words.\"), Scene(scene_type='mid', title='Emergent Behavior 2: The Loop Assembly Line', visual_description=\"New Scenario: The task is 'Summarize every chapter'. Regex won't work here. The text block is shown as a long segmented bar. The Model writes a Python `for` loop: `for chapter in text.split('\\\\n\\\\n'):`. \\n\\nAnimation: The loop activates. The long bar is sliced into 50 small squares. The Model creates a small clone of itself (a Sub-LM). The loop grabs the first square, feeds it to the Sub-LM, gets a summary, and places the summary in a 'Result' bucket. This repeats rapidly: Grab, Process, Store. The original Model never touches the raw text.\", narration=\"But what if the answer requires reading everything? The model spontaneously reinvented the 'for loop'. It breaks the massive text into bite-sized chunks—say, by chapter or paragraph. It then spins up smaller instances of itself to process each chunk one by one, aggregating the results. It treats the text like a data stream.\"), Scene(scene_type='mid', title='Emergent Behavior 3: Self-Verification', visual_description=\"The Model is holding a tentative answer: 'The date was 1994'. It looks uncertain. Instead of outputting it, the Model puts it in a variable called `candidate`. It writes new code: `if '1994' in context[500:600]: return True`. \\n\\nThe code executes. A green checkmark appears over the answer. Only THEN does the Model output the final result to the user.\", narration=\"Perhaps the most impressive behavior is self-doubt. We've seen models generate an answer, hold it in memory, and then write a completely new script to verify that answer against the original text before showing it to the user. It's fact-checking itself in real-time.\"), Scene(scene_type='closing', title='The Efficiency Curve', visual_description=\"A graph appears. X-axis: Input Length (up to 10M). Y-axis: Compute Cost. \\n\\nA red line (Standard LLM) shoots straight up, hitting a ceiling and exploding. \\n\\nA blue line (RLM) stays low and flat. \\n\\nVisual icons of the 'Tools' (The Searchlight, The Loop, The Checkmark) appear along the blue line, propping it up / keeping it low.\", narration=\"These aren't just cool tricks; they are survival strategies. By using code to filter, loop, and verify, the model avoids processing irrelevant information. It transforms a reasoning problem into a data processing problem, allowing it to handle contexts that are effectively infinite.\")]),\n",
       " 'Performance Results and Cost Analysis': TopicStoryboard(topic_name='Performance Results and Cost Analysis', visual_concept=\"The central metaphor is a 'Searchlight vs. Floodlight' comparison. Standard models are shown as Floodlights that must illuminate (process) the entire landscape (context) at high cost to see anything. The RLM is a Searchlight, controlled by code, that selectively illuminates only relevant distinct patches. We use bar charts that break through ceilings to show performance gains, and 'receipt' animations to visualize the surprising cost savings.\", scenes=[Scene(scene_type='hook', title='The Recursive Cost Myth', visual_description=\"A dark screen. A glowing green line draws a recursive tree structure, branching rapidly outward until it fills the screen, implying complexity and explosion. A dollar sign '$' counter in the corner spins rapidly up to a huge number. Suddenly, a blue 'Code' bracket appears around the tree. The tree instantly prunes itself down to a single, sleek path. The dollar counter spins back down to a low number. Text appears: 'More Steps ≠ More Cost?'.\", narration=\"When we hear 'recursive' or 'looping' models, we instinctively clutch our wallets. The assumption is simple: running a model multiple times must be exponentially more expensive than running it once. But the data reveals a surprising paradox.\"), Scene(scene_type='mid', title='The 10 Million Token Challenge', visual_description=\"We transition to the 'BrowseComp-Plus' benchmark. Visual: A massive stack of 1,000 digital documents appears on the left. A standard robot (labeled 'Base GPT-5') tries to ingest them all at once. The robot turns red and displays a 'CONTEXT LIMIT' warning symbol. A bar chart appears. The bar for 'Base Model' sits flat at 0%. A second bar for 'CodeAct Agent' rises to 51%.\", narration=\"Let's look at the BrowseComp-Plus benchmark. The task? Answer questions requiring reasoning across one thousand documents—over ten million tokens. For a standard base model, even a powerful one like GPT-5, this is a non-starter. It simply cannot fit the context. Even standard agents only get it right about half the time.\"), Scene(scene_type='mid', title='RLM Performance Breakthrough', visual_description=\"The previous bar chart remains. A new bar labeled 'RLM (GPT-5)' shoots up aggressively, towering over the others. It hits a gold line labeled '91.33%'. Confetti particles explode gently from the top of the bar. To the right, we show the 'OOLONG-Pairs' task (dense reasoning). A similar graph: Base Model is a tiny sliver (<1%). The RLM bar rises significantly to 58%.\", narration='But when we equip that same model with a Recursive Loop, accuracy skyrockets to over ninety-one percent. And on tasks requiring incredibly dense, quadratic reasoning—where the base model scores near zero—the RLM effectively solves the impossible.'), Scene(scene_type='mid', title='The Floodlight vs. Searchlight', visual_description=\"Split screen. Left side (Base Model): A floodlight turns on, trying to light up a massive warehouse (the context). It's bright but consumes a massive battery meter instantly. Right side (RLM): The warehouse is dark. A thin, precise laser beam (the RLM) flicks on, illuminating just one box, then turns off. Moves to another box, turns on. The battery meter on the right drains very slowly.\", narration=\"How is this possible without breaking the bank? Think of a standard model as a floodlight. To find a needle, it has to illuminate the entire haystack at once. That's expensive.\"), Scene(scene_type='mid', title='Code as a Filter', visual_description=\"An animation of the RLM process. A document stream flows horizontally. A Python code block appears: 'if keyword in text: keep()'. The code acts as a gate. 90% of the text flows past the gate and fades away (greyed out). Only 10% (highlighted in gold) enters the model's 'Processing Unit'. A equation appears: 'Cost = Processed Tokens', showing that Total Tokens >> Processed Tokens.\", narration=\"The Recursive Language Model acts more like a flashlight in a dark room. It uses Python code to 'peek' at the data first. It filters out the noise programmatically—which is free—and only pays to process the exact paragraphs that matter.\"), Scene(scene_type='mid', title='The Cost Receipt', visual_description=\"A digital receipt prints out on screen. Item 1: 'GPT-5 Full Context Reading'. Price: '$1.50 - $2.75'. Item 2: 'RLM Selective Reading'. Price: '$0.99'. A 'SAVINGS' stamp slams onto the RLM receipt. Beside this, a scatter plot appears showing cost variance. RLM dots are mostly low, with a few high outliers, while Base Model dots form a high, solid horizontal line.\", narration=\"This selectivity translates to real dollars. In the experiments, ingesting the full context cost up to two dollars and seventy-five cents per query. The RLM? It averaged just ninety-nine cents. It's not just better; for massive contexts, it's actually cheaper.\"), Scene(scene_type='mid', title='Why Summary Fails', visual_description=\"Comparison with 'Summary Agents'. Visual: A photocopier copying a document, then copying the copy, then copying that copy. The final image is blurry and unreadable. Next to it, the RLM concept: A magnifying glass looking directly at the original page. Text label: 'Lossy Compression' vs 'Direct Access'.\", narration=\"You might ask, 'Why not just summarize the text to make it fit?' The data shows this fails. Summarization is 'lossy'—you lose details. The RLM doesn't compress; it hunts. It keeps the original data pristine but only looks at what it needs.\"), Scene(scene_type='closing', title='The Efficiency Frontier', visual_description=\"A 2D coordinate plane. X-axis is 'Cost', Y-axis is 'Performance'. We plot points for 'Retrieval', 'Summarization', and 'Base LLM'. They form a curve. The 'RLM' point appears in the top-left quadrant (High Performance, Low Cost), breaking the expected trade-off curve. The RLM point pulses and connects to the others with dotted lines, showing the gap.\", narration=\"For years, we've assumed a strict trade-off: higher accuracy requires more compute, and thus higher cost. Recursive Language Models break this curve. They demonstrate that the smartest way to read a book isn't to memorize every page—it's to know exactly which page to turn to.\")])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storyboards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdd089a",
   "metadata": {},
   "source": [
    "# Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abfdce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_client = ChatGoogleGenerativeAI(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=1.0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "langchain_client.client = gemini_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ca54b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=[{'type': 'text', 'text': 'Hello! How can I help you today?', 'extras': {'signature': 'Et0JCtoJAXLI2nwX7T71wo5hLUrtVapOkl0Xc/ylVgFrKl3Xlf90bhk6mjafv4aeJaWWG9n7Vy8JhdvHzUap1Xvtd51yC/EIuxYw9/L1RVrGJ9FrwtLsZu/+qIuRZd8DowX31mLFAy+6gziuqxqmP5t52dHwt62ybTAOVtCbefFO3MbS8Y6FEKFQjn6XWgPh5ZZZmlzVvygvjVHlQD/Q10xacoKDWJsU/QMp2a+VPmSk8GEQ0CU4UzSrJorzrUyjLcqDnbT08VQks8fPvJKX+18q1Xy7dHEChe3aOzVJRPyprDxc1BrwOm/IzojiOjy8ZaWM920wWfIOa6lwxdvfDP0aQ/ySpfHxk688kkZ41dhFfVanh82qv7wpk2w3MbtyUYiDAvSeGT0eqzwobAAjaHwxWMVOwH1EGm69OO1tWmnI60wLdyyusppZeoQ9PHhHdn1lNzc8RxVJqDUmsr+BuvOOSU2h2YcfgA9J7fDNLtrdd7G+7bqMBeKqDVD570DkAMbb1yKZTZtd6//GMqZqiuL+zoImW6jd2Ck4iSxpRfaNeem9NF/B8dffZeo1uVzHz4zMzFcW0W6HnjkreCnB0p0jEOgPWAfxeGZg0BabpEabnLFpbq0sfLiLNKqiYXH6cC09nvZNq8lPC8Dqy/zOJJN+8hszC4Ow/tJel97y3Ku40lDJVIdAu4UGvRXR98yxdf1MeYdGxWU8SUQgQRWduaTMSt7b1ZlBTlGDdOqQ4+rsBEvlNa0MhefP3YXJ5jDCR9bxiXf22crPoEewwORLqQJRzLqyewduH8whdgzpmwJ6MrQMQ2zdoFUlg39nniI4LEUVNiUvDSVvcqF7nlwoampFG77+X4tEvxQe8nibSU/m3sF9tQ1+FzMBSNq+HTdjtKIlLDQJvdM3Xy/NBd+8Twtx7TxrwqzAztT1MI7t8so4Zcgzqh2ZFlAtPMd7EnuIZYa1kNsbkHezF1OrhKbNMbvI0b2/9JiDXnRR0cbo9b8eC3qyWkUZWQ7lDTiB5m011T55pJI1GuNg9hgZqmQp79xPyRHlfCUPJfTqx+ozDvtqVziGv9fARUzC3XAU5jBe4G0/5mQctpkaycd+800Aj7jRYnywJtcZREJ0tulqMqvXCL0q9DLHscTCoVwN1QfX2EC2v3iXcyhneMekF99Fp1obZCg+zN2uWt2HH9RQ0NF23kJ8MFUW1KgP7iQPCFpzuarFCCbMfcup0xJBH55L9txaXi9jJMxJWEGp3NjP6qyi1XMigcNtKT6m0UsGFkZRdtA12h3zfut2xxJDqrritiMzA2uTFkN4eFmMtLg3m/cN/mLsxXq8G5PvIG6JLESj61j5sfgUff2tvHy/Nk6fFK8NHQI7mUuSQ08k+06VWph6cDy+XHzLkTNkgtetTEaB0Xj1JLiMkK4qhzTlqhF1wIl40fuQuCJrxI3VNgMXiZLKv3JCj5SVXut9u5bA4RmFYV2rcESHXefjRVNubjL52LXxb5npBzVoynqwAx+rYYb0OQua4CSpcbDJgIsKaxxZuGx8YjPNf6dmFOICgQUroe3LVSqNUAOoAZRFO5vG+jCbiQpdVYPdBaOQzClfPUJb0Uo2uje6+w+4hugKcgOyOsiMnK9EucekusnRatE6ginjuc6UoWiWTU6ci/X3iiqM'}}], additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-3-pro-preview', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b9a3c-c062-7e30-9de2-bd76f8fe7eed-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 5, 'output_tokens': 331, 'total_tokens': 336, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 322}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_client.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec57e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eduly_animation_client = EdulyAnimationClient(langchain_client, agent_workspace_path='./agent_workspace/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebb4b33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The Problem of Context Rot in Long-Context LLMs': TopicStoryboard(topic_name='The Problem of Context Rot in Long-Context LLMs', visual_concept=\"The central visual metaphor is 'The Overloaded Workbench.' We contrast a model trying to memorize an entire library at once (context rot) versus a model sitting at a desk, fetching only the specific books it needs when it needs them (inference-time scaling/out-of-core algorithms).\", scenes=[Scene(scene_type='hook', title='The Infinite Scroll', visual_description=\"A sleek, glowing digital brain (representing an LLM) sits in the center. A stream of documents, books, and code enters its 'mind' from the left. A counter above the head counts up rapidly: '10,000 tokens', '100,000 tokens', '1,000,000 tokens'. The brain glows brighter, looking powerful. The background is a clean, dark void with grid lines.\", narration='In the world of artificial intelligence, we are witnessing an explosion of memory. Modern language models can now technically ingest entire libraries—millions of words—in a single prompt. It sounds like the ultimate super-power: perfect recall at an infinite scale.'), Scene(scene_type='mid', title='The Fog of Data', visual_description=\"The camera zooms into the brain. Inside, we see a representation of 'Attention'—lines connecting different words. At first, with few tokens, the lines are sharp and bright gold. As the token counter continues to rise to 1 million, the lines become a chaotic, tangled mess. The bright gold connections fade into a murky, grey fog. A red warning label appears: 'CONTEXT ROT'.\", narration=\"But there is a catch. Just because a model can *swallow* a million words, doesn't mean it can *digest* them. As the input grows, a phenomenon called 'Context Rot' sets in. The model's attention gets diluted, drowning in the noise of its own memory.\"), Scene(scene_type='mid', title='Needles vs. Haystacks', visual_description=\"Split screen. \\nLeft side: 'Simple Task'. A massive pile of hay. One glowing red needle. A spotlight scans the hay and instantly finds the needle. A checkmark appears.\\nRight side: 'Dense Task'. The same hay pile, but now there are hundreds of red threads woven together intricately. The spotlight tries to follow the pattern but gets lost, flickering and dimming. An 'X' appears.\", narration=\"It's not that the models simply forget. They can easily find a specific password hidden in a book—that's the 'needle in a haystack' problem. But ask them to track a complex plot or connect an idea from page one to page five hundred? That's where they crumble.\"), Scene(scene_type='mid', title='The Performance Cliff', visual_description=\"A 2D line graph appears. \\nX-axis: 'Input Length (Tokens)' (Log scale: 8k, 128k, 1M).\\nY-axis: 'Reasoning Accuracy'.\\nTwo lines are drawn. \\nLine 1 (Green, labeled 'Retrieval'): Stays flat and high.\\nLine 2 (Red, labeled 'Deep Reasoning'): Starts high, but plummets aggressively as it passes 100k tokens, crashing near zero.\", narration=\"This degradation isn't subtle. On tasks that require reasoning across the whole text, performance doesn't just dip—it dives off a cliff. The model's *physical* context window is huge, but its *effective* reasoning window is surprisingly small.\"), Scene(scene_type='mid', title='The Memory Bottleneck', visual_description=\"We transition to a computer hardware analogy. \\nOn the left: A small, fast microchip labeled 'RAM' (Context Window). \\nOn the right: A massive stack of hard drives labeled 'Environment' (The Prompt).\\nAn animation shows a user trying to shove the entire hard drive stack into the tiny RAM chip. The chip smokes and turns red.\", narration=\"Think of it like a computer. Trying to stuff a million-token prompt into the model's active attention is like trying to load a terabyte database entirely into your RAM. It's inefficient, expensive, and frankly, overwhelming.\"), Scene(scene_type='mid', title='Out-of-Core Inspiration', visual_description=\"The smoking chip is replaced by a calm, rhythmic system. \\nData flows from the 'Hard Drive' (Prompt) to the 'RAM' (Model) in small, neat packets. \\nThe Model processes a packet, writes a note, discards the packet, and requests the next one. \\nText appears: 'Out-of-Core Algorithm'.\", narration=\"Computer science solved this decades ago with 'out-of-core' algorithms. If a dataset is too big for memory, you don't crash. You leave the data on the disk, and you fetch only the chunks you need, when you need them.\"), Scene(scene_type='mid', title='Inference-Time Scaling', visual_description=\"Now we apply this to the LLM. \\nThe 'Prompt' is visualized as a long scroll sitting on a desk (the 'Environment'). \\nThe LLM appears as a robotic hand holding a pen (a Python Interpreter). \\nThe hand writes code: `read(chapter_1)`, processes it, then writes `read(chapter_5)`. \\nThe massive scroll never enters the robot's head all at once.\", narration=\"This is the core idea behind 'Inference-Time Scaling.' Instead of feeding the prompt *into* the neural network, we treat the prompt as an external environment. The model uses code to browse the text, read specific snippets, and reason iteratively.\"), Scene(scene_type='closing', title='The Flat Line of Success', visual_description='We return to the graph from Scene 4. \\nThe red crashing line (Base Model) is still there. \\nA new Blue Line (Recursive Model) animates. It follows the red line initially, but where the red line crashes, the blue line stays perfectly horizontal—maintaining high accuracy even at 1 million tokens. \\nConfetti or sparkles erupt along the high plateau of the blue line.', narration=\"The result? We stop the rot. By processing information in manageable pieces, we can extend the reasoning capabilities of models by orders of magnitude. It turns out the secret to reading a long book isn't a bigger brain—it's learning how to turn the pages.\")]),\n",
       " 'Recursive Language Models (RLMs): The Core Architecture': TopicStoryboard(topic_name='Recursive Language Models (RLMs): The Core Architecture', visual_concept=\"We visualize the RLM as a 'Model-in-the-Loop' system. The central metaphor is a 'programmer' (the Root LM) sitting at a terminal (the REPL) separated from a massive library (the Prompt) by a glass wall. Instead of trying to stuff the library into its head, the programmer types commands to retrieve specific pages or dispatch assistants (Sub-LMs) to summarize sections, building the answer piece by piece.\", scenes=[Scene(scene_type='hook', title='The Context Bottleneck', visual_description=\"A standard Transformer architecture is depicted on the left as a complex network of nodes. On the right, a massive scroll of text (representing a book or codebase) attempts to feed into the model's input. As the scroll gets longer, it hits a glowing red barrier labeled 'Context Window Limit'. The excess text piles up and burns away, symbolizing lost information. A counter shows 'Tokens: 200k... 500k... 10M', and the model flashes 'Out of Memory'.\", narration='Imagine trying to memorize an entire library in a single glance. Even the most powerful AI models today face a hard limit: the context window. Feed them too much text, and they either crash or simply forget the beginning by the time they reach the end. We need a fundamental shift in how we handle massive inputs.'), Scene(scene_type='mid', title='Flipping the Perspective', visual_description=\"The scene wipes clean. The massive text scroll reappears, but now it is placed inside a distinct, box-like container labeled 'Environment (Python REPL)'. The Language Model (Root LM) sits *outside* this box. There is no direct pipe feeding the text into the LM. Instead, a thin line connects the LM to the Environment box, labeled 'Code Interface'.\", narration='Recursive Language Models, or RLMs, solve this by flipping the script. Instead of feeding the prompt directly into the neural network, the RLM treats the text as part of an external environment. Think of it less like reading a book, and more like managing a database.'), Scene(scene_type='mid', title='The Variable Abstraction', visual_description=\"Inside the 'Environment' box, the massive text scroll condenses into a single block labeled with the variable name `prompt`. A metadata bubble floats up to the Root LM: 'Variable: prompt, Type: String, Length: 10,000,000'. The Root LM displays a thought bubble containing Python syntax.\", narration=\"When an RLM starts, it doesn't see the text. It sees a variable. It knows it has a string called 'prompt' and it knows that string is ten million characters long. But the content remains inside the Python environment, safe and accessible, without clogging the model's brain.\"), Scene(scene_type='mid', title=\"The Programmer's Interface\", visual_description=\"The Root LM emits a line of code: `print(prompt[:200])`. This code travels down the line into the Environment. The REPL executes it. A small snippet of text ('Chapter 1: The beginning...') pops out of the `prompt` block and travels back up to the Root LM. The LM stores this small observation in its local context.\", narration='The model acts as a programmer. If it wants to know how the story starts, it writes a line of code to slice the string. The environment executes the code and returns just that specific snippet. The model can peek, probe, and navigate the data efficiently.'), Scene(scene_type='mid', title='The Recursive Call', visual_description='The Root LM writes more complex code: `chunk = prompt[5000:6000]` followed by `summary = llm_query(chunk)`. Inside the Environment box, a smaller, semi-transparent version of the neural network appears (the Sub-LM). It consumes the `chunk`, processes it, and turns it into a small summary node. This summary is returned to the Root LM.', narration='But here is the true superpower: recursion. The model can write code that calls *itself*. It can carve out a specific chapter and pass it to a sub-instance of the model, asking it to summarize or analyze just that piece. The sub-model does the heavy lifting and returns a concise result.'), Scene(scene_type='mid', title='The Loop (Read-Eval-Print)', visual_description=\"A cyclic animation begins. 1. Root LM generates code. 2. Environment executes code (sometimes spawning Sub-LMs). 3. Output returns to Root LM. 4. Root LM updates its state. The cycle repeats rapidly. As it loops, a 'Final Answer' document constructs itself on the side, line by line, derived from these interactions.\", narration=\"This creates a loop. Read, Evaluate, Print. The model programs its own investigation, delegating tasks to sub-models, aggregating their findings, and deciding what to check next. It's not just reading; it's actively reasoning over the data.\"), Scene(scene_type='closing', title='Infinite Scaling', visual_description=\"We zoom out to show the `prompt` variable growing larger and larger—100 million tokens, 1 billion tokens. The Root LM remains the same size, calm and unburdened. The 'Code Interface' line pulses steadily, handling the massive scale with ease. The background transitions to a vast library being managed by a single, efficient librarian.\", narration=\"By decoupling the input size from the model's architecture, RLMs theoretically unlock infinite context. The limit is no longer how much the model can think about at once, but simply how much memory the computer environment can hold. It turns the passive reader into an active explorer.\")]),\n",
       " 'Task Complexity and Information Density': TopicStoryboard(topic_name='Task Complexity and Information Density', visual_concept=\"We visualize the input text as a stream of data blocks. We classify tasks by how 'dense' the connections between these blocks are. 'Constant' complexity is finding one glowing block. 'Linear' complexity is processing every block individually (like an assembly line). 'Quadratic' complexity is drawing a web of connecting lines between every possible pair of blocks, illustrating why standard models choke on the computational load while RLMs handle it systematically.\", scenes=[Scene(scene_type='hook', title='The Context Rot Cliff', visual_description=\"A 2D graph appears. The X-axis is 'Context Length' (log scale, 8k to 1M tokens). The Y-axis is 'Performance'. Three lines appear. \\n1. Top line (Blue): A flat, high line labeled 'Finding a Needle'. \\n2. Middle line (Orange): Starts high, slowly dips. Labeled 'Summarizing'.\\n3. Bottom line (Red): Starts high, then plummets to near zero rapidly. Labeled 'Pairwise Reasoning'.\\n\\nA spotlight highlights the 'Red Zone' where the Red line crashes. The text 'Context Rot' fades in over the drop.\", narration=\"We often ask: how much text can an AI read? But that's the wrong question. The real question is: what do you want it to *do* with that text? Not all long documents are created equal. Depending on the task, a model's brain might break down at ten thousand words, or sail smoothly past a million.\"), Scene(scene_type='mid', title='Defining Information Density', visual_description=\"The screen clears. A long horizontal stream of gray data blocks moves from right to left, representing a long prompt. A magnifying glass appears. \\nThree icons appear below the stream:\\n1. A single Diamond (S-NIAH)\\n2. A row of Bricks (OOLONG)\\n3. A complex Spiderweb (OOLONG-Pairs)\\n\\nAn arrow points from 'Input Size' to 'Processing Cost', changing thickness based on the icon selected.\", narration='The paper introduces the concept of Information Density. It categorizes tasks by how hard they are to compute as the document gets longer. We call these Constant, Linear, and Quadratic complexity.'), Scene(scene_type='mid', title='Level 1: The Needle (Constant)', visual_description=\"Focus on the Diamond icon. The stream of gray blocks pauses. Somewhere deep in the middle, one single block glows bright cyan. \\nA standard 'Attention Spotlight' scans frantically back and forth.\\nThen, the view splits. On the right, a Python terminal appears. It types `text.find('secret_code')`. It instantly snaps to the cyan block without scanning the rest.\", narration=\"First, the simplest level: Needle in a Haystack. You're looking for one specific phrase, like a password. Whether the book is ten pages or a thousand, the answer is just one tiny piece. Standard models are okay at this, but a Recursive Language Model is smarter—it doesn't read; it uses code, like a search function, to jump straight to the answer.\"), Scene(scene_type='mid', title='Level 2: The Assembly Line (Linear)', visual_description=\"Focus on the Brick row icon. The stream of gray blocks transforms so that *every* block has a unique number on it (10, 5, 30, 2...). \\nTask text appears: 'Sum all numbers'.\\n\\nMethod A (Summarization): A giant trash compactor labeled 'Summarizer' crushes the blocks into a small, blurry cube. The numbers are unreadable. Result: 'Approx 50?'.\\n\\nMethod B (RLM): The blocks move along a conveyor belt. A robotic arm (The Sub-LM) picks up one block, reads it, adds it to a running total on a scoreboard, and tosses it. It processes 100% of the blocks one by one.\", narration='Next is Linear complexity. Here, you need to process every single sentence to get the answer—like summing numbers in a ledger. Summarization agents fail here because summarizing compresses the data, blurring out the specific details you need. But an RLM treats this like a for-loop. It processes chunk by chunk, aggregating the data without losing a single digit.'), Scene(scene_type='mid', title='Level 3: The Web (Quadratic)', visual_description=\"Focus on the Spiderweb icon. The stream of blocks appears again. \\nTask text: 'How does every character relate to every other character?'.\\n\\nLines begin to draw from the first block to *every other block* in the sequence. Then from the second block to every other block. \\nThe screen rapidly fills with a dense, messy mesh of lines. \\nA standard AI model icon (a brain) appears, overheats, and turns red (representing the crash in Figure 1).\", narration=\"This is the boss level: Quadratic complexity. The 'Pairwise' task. Here, you aren't just reading chunks; you have to compare every chunk to every other chunk. If you double the document length, the work doesn't just double—it quadruples. Frontier models like GPT-5 crash completely here. Their attention mechanisms just can't hold that many connections at once.\"), Scene(scene_type='mid', title=\"RLM's Quadratic Solution\", visual_description=\"The messy web clears. We see the Python terminal again. \\nCode appears: \\n`for i in chunks:`\\n`  for j in chunks:`\\n`    compare(i, j)`\\n\\nThe visualization shows two pointers. The 'i' pointer stays on Block 1. The 'j' pointer slides across all other blocks, drawing a clean, single line for each comparison. Then 'i' moves to Block 2, and 'j' scans again. It is slow, methodical, and organized.\", narration=\"This is where the Recursive Language Model proves its worth. It doesn't try to hold the whole web in its head. It writes a nested loop. It compares piece A to piece B, then piece A to piece C, systematically working through the pairs. It takes more time, but unlike the standard model which scores nearly zero, the RLM actually solves the problem.\"), Scene(scene_type='closing', title='The Right Tool for the Job', visual_description=\"Split screen into three horizontal bands.\\nTop: A search bar finding a needle (Constant).\\nMiddle: A conveyor belt processing items (Linear).\\nBottom: A matrix grid filling up cell by cell (Quadratic).\\n\\nText overlay: 'RLM adapts the strategy to the density.'\", narration=\"The key insight is that 'Long Context' isn't just one problem. It's a spectrum of density. By offloading the memory management to code—loops, variables, and searches—Recursive Language Models adapt their strategy to match the complexity of the task, succeeding where biological-style attention fails.\")]),\n",
       " 'Emergent Behaviors: How RLMs Actually Reason': TopicStoryboard(topic_name='Emergent Behaviors: How RLMs Actually Reason', visual_concept=\"The RLM is depicted as a 'Master Architect' sitting at a workbench (the REPL). Instead of trying to memorize a massive blueprint (the prompt) all at once, the Architect uses specific tools—a magnifying glass (Regex), a pair of scissors (Chunking), and a checklist (Verification)—to manipulate the blueprint externally. The code blocks appear as glowing blue instructions that animate the tools.\", scenes=[Scene(scene_type='hook', title='The Impossible Memory Test', visual_description='A massive, towering stack of papers (representing 10 million tokens) looms over a small, standard brain icon. The brain tries to absorb it all, glowing red and shaking, indicating overload. Suddenly, the stack is pushed aside. An identical brain appears, but this one is sitting at a desk with a computer terminal. It looks calm. On the terminal screen, a single line of code appears: \\'search(\"date\", document)\\'. The massive stack instantly vanishes, leaving only a single sheet of paper floating in the air.', narration=\"Imagine asking a standard AI to memorize a library's worth of text just to find one specific date. It would likely crash or hallucinate. But Recursive Language Models don't try to memorize. They do something much more human: they cheat. Well, they use tools.\"), Scene(scene_type='mid', title='The Environment as a Workspace', visual_description=\"The screen splits. On the left is the 'Model' (a glowing node network). On the right is the 'Environment' (a dark terminal window). A long scroll labeled 'PROMPT_VAR' sits inside the terminal, NOT inside the Model. The Model shoots a beam of light (code) into the terminal. The code reads: `len(PROMPT_VAR)`. The terminal returns `10,000,000`. The Model nods. It never pulls the full scroll into itself.\", narration=\"The key is that the long text isn't fed directly into the model's brain. It sits in a Python environment as a variable. The model just looks at the variable name, realizes it's too big to read, and decides to write code to peek at it instead.\"), Scene(scene_type='mid', title='Emergent Behavior 1: The Regex Searchlight', visual_description=\"We zoom into the terminal. The text variable is a long, blurry gray block. The Model generates code: `re.search(r'Festival.*La Union', text)`. As this code executes, a bright yellow 'searchlight' beam scans across the gray text block. It ignores 99% of the text, moving rapidly. It stops and illuminates just two lines: '...annual arts festival held in La Union...' The rest of the text fades to black. The Model absorbs only these two lines.\", narration=\"The first surprise behavior is filtering. Without being explicitly trained to do so, these models started writing Regular Expressions. Like a researcher using 'Control-F', they use keyword searches to pinpoint exactly what they need, ignoring millions of irrelevant words.\"), Scene(scene_type='mid', title='Emergent Behavior 2: The Loop Assembly Line', visual_description=\"New Scenario: The task is 'Summarize every chapter'. Regex won't work here. The text block is shown as a long segmented bar. The Model writes a Python `for` loop: `for chapter in text.split('\\\\n\\\\n'):`. \\n\\nAnimation: The loop activates. The long bar is sliced into 50 small squares. The Model creates a small clone of itself (a Sub-LM). The loop grabs the first square, feeds it to the Sub-LM, gets a summary, and places the summary in a 'Result' bucket. This repeats rapidly: Grab, Process, Store. The original Model never touches the raw text.\", narration=\"But what if the answer requires reading everything? The model spontaneously reinvented the 'for loop'. It breaks the massive text into bite-sized chunks—say, by chapter or paragraph. It then spins up smaller instances of itself to process each chunk one by one, aggregating the results. It treats the text like a data stream.\"), Scene(scene_type='mid', title='Emergent Behavior 3: Self-Verification', visual_description=\"The Model is holding a tentative answer: 'The date was 1994'. It looks uncertain. Instead of outputting it, the Model puts it in a variable called `candidate`. It writes new code: `if '1994' in context[500:600]: return True`. \\n\\nThe code executes. A green checkmark appears over the answer. Only THEN does the Model output the final result to the user.\", narration=\"Perhaps the most impressive behavior is self-doubt. We've seen models generate an answer, hold it in memory, and then write a completely new script to verify that answer against the original text before showing it to the user. It's fact-checking itself in real-time.\"), Scene(scene_type='closing', title='The Efficiency Curve', visual_description=\"A graph appears. X-axis: Input Length (up to 10M). Y-axis: Compute Cost. \\n\\nA red line (Standard LLM) shoots straight up, hitting a ceiling and exploding. \\n\\nA blue line (RLM) stays low and flat. \\n\\nVisual icons of the 'Tools' (The Searchlight, The Loop, The Checkmark) appear along the blue line, propping it up / keeping it low.\", narration=\"These aren't just cool tricks; they are survival strategies. By using code to filter, loop, and verify, the model avoids processing irrelevant information. It transforms a reasoning problem into a data processing problem, allowing it to handle contexts that are effectively infinite.\")]),\n",
       " 'Performance Results and Cost Analysis': TopicStoryboard(topic_name='Performance Results and Cost Analysis', visual_concept=\"The central metaphor is a 'Searchlight vs. Floodlight' comparison. Standard models are shown as Floodlights that must illuminate (process) the entire landscape (context) at high cost to see anything. The RLM is a Searchlight, controlled by code, that selectively illuminates only relevant distinct patches. We use bar charts that break through ceilings to show performance gains, and 'receipt' animations to visualize the surprising cost savings.\", scenes=[Scene(scene_type='hook', title='The Recursive Cost Myth', visual_description=\"A dark screen. A glowing green line draws a recursive tree structure, branching rapidly outward until it fills the screen, implying complexity and explosion. A dollar sign '$' counter in the corner spins rapidly up to a huge number. Suddenly, a blue 'Code' bracket appears around the tree. The tree instantly prunes itself down to a single, sleek path. The dollar counter spins back down to a low number. Text appears: 'More Steps ≠ More Cost?'.\", narration=\"When we hear 'recursive' or 'looping' models, we instinctively clutch our wallets. The assumption is simple: running a model multiple times must be exponentially more expensive than running it once. But the data reveals a surprising paradox.\"), Scene(scene_type='mid', title='The 10 Million Token Challenge', visual_description=\"We transition to the 'BrowseComp-Plus' benchmark. Visual: A massive stack of 1,000 digital documents appears on the left. A standard robot (labeled 'Base GPT-5') tries to ingest them all at once. The robot turns red and displays a 'CONTEXT LIMIT' warning symbol. A bar chart appears. The bar for 'Base Model' sits flat at 0%. A second bar for 'CodeAct Agent' rises to 51%.\", narration=\"Let's look at the BrowseComp-Plus benchmark. The task? Answer questions requiring reasoning across one thousand documents—over ten million tokens. For a standard base model, even a powerful one like GPT-5, this is a non-starter. It simply cannot fit the context. Even standard agents only get it right about half the time.\"), Scene(scene_type='mid', title='RLM Performance Breakthrough', visual_description=\"The previous bar chart remains. A new bar labeled 'RLM (GPT-5)' shoots up aggressively, towering over the others. It hits a gold line labeled '91.33%'. Confetti particles explode gently from the top of the bar. To the right, we show the 'OOLONG-Pairs' task (dense reasoning). A similar graph: Base Model is a tiny sliver (<1%). The RLM bar rises significantly to 58%.\", narration='But when we equip that same model with a Recursive Loop, accuracy skyrockets to over ninety-one percent. And on tasks requiring incredibly dense, quadratic reasoning—where the base model scores near zero—the RLM effectively solves the impossible.'), Scene(scene_type='mid', title='The Floodlight vs. Searchlight', visual_description=\"Split screen. Left side (Base Model): A floodlight turns on, trying to light up a massive warehouse (the context). It's bright but consumes a massive battery meter instantly. Right side (RLM): The warehouse is dark. A thin, precise laser beam (the RLM) flicks on, illuminating just one box, then turns off. Moves to another box, turns on. The battery meter on the right drains very slowly.\", narration=\"How is this possible without breaking the bank? Think of a standard model as a floodlight. To find a needle, it has to illuminate the entire haystack at once. That's expensive.\"), Scene(scene_type='mid', title='Code as a Filter', visual_description=\"An animation of the RLM process. A document stream flows horizontally. A Python code block appears: 'if keyword in text: keep()'. The code acts as a gate. 90% of the text flows past the gate and fades away (greyed out). Only 10% (highlighted in gold) enters the model's 'Processing Unit'. A equation appears: 'Cost = Processed Tokens', showing that Total Tokens >> Processed Tokens.\", narration=\"The Recursive Language Model acts more like a flashlight in a dark room. It uses Python code to 'peek' at the data first. It filters out the noise programmatically—which is free—and only pays to process the exact paragraphs that matter.\"), Scene(scene_type='mid', title='The Cost Receipt', visual_description=\"A digital receipt prints out on screen. Item 1: 'GPT-5 Full Context Reading'. Price: '$1.50 - $2.75'. Item 2: 'RLM Selective Reading'. Price: '$0.99'. A 'SAVINGS' stamp slams onto the RLM receipt. Beside this, a scatter plot appears showing cost variance. RLM dots are mostly low, with a few high outliers, while Base Model dots form a high, solid horizontal line.\", narration=\"This selectivity translates to real dollars. In the experiments, ingesting the full context cost up to two dollars and seventy-five cents per query. The RLM? It averaged just ninety-nine cents. It's not just better; for massive contexts, it's actually cheaper.\"), Scene(scene_type='mid', title='Why Summary Fails', visual_description=\"Comparison with 'Summary Agents'. Visual: A photocopier copying a document, then copying the copy, then copying that copy. The final image is blurry and unreadable. Next to it, the RLM concept: A magnifying glass looking directly at the original page. Text label: 'Lossy Compression' vs 'Direct Access'.\", narration=\"You might ask, 'Why not just summarize the text to make it fit?' The data shows this fails. Summarization is 'lossy'—you lose details. The RLM doesn't compress; it hunts. It keeps the original data pristine but only looks at what it needs.\"), Scene(scene_type='closing', title='The Efficiency Frontier', visual_description=\"A 2D coordinate plane. X-axis is 'Cost', Y-axis is 'Performance'. We plot points for 'Retrieval', 'Summarization', and 'Base LLM'. They form a curve. The 'RLM' point appears in the top-left quadrant (High Performance, Low Cost), breaking the expected trade-off curve. The RLM point pulses and connects to the others with dotted lines, showing the gap.\", narration=\"For years, we've assumed a strict trade-off: higher accuracy requires more compute, and thus higher cost. Recursive Language Models break this curve. They demonstrate that the smartest way to read a book isn't to memorize every page—it's to know exactly which page to turn to.\")])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storyboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c86dd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['The Problem of Context Rot in Long-Context LLMs', 'Recursive Language Models (RLMs): The Core Architecture', 'Task Complexity and Information Density', 'Emergent Behaviors: How RLMs Actually Reason', 'Performance Results and Cost Analysis'])\n"
     ]
    }
   ],
   "source": [
    "print(storyboards.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "467bb84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Problem of Context Rot in Long-Context LLMs\n",
      "0 The Infinite Scroll\n",
      "1 The Fog of Data\n",
      "2 Needles vs. Haystacks\n",
      "3 The Performance Cliff\n",
      "4 The Memory Bottleneck\n",
      "5 Out-of-Core Inspiration\n",
      "6 Inference-Time Scaling\n",
      "7 The Flat Line of Success\n"
     ]
    }
   ],
   "source": [
    "test_storyboard_0 = storyboards['The Problem of Context Rot in Long-Context LLMs']\n",
    "print(test_storyboard_0.topic_name)\n",
    "for i, scene in enumerate(test_storyboard_0.scenes):\n",
    "    print(i, scene.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f689c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Topic 0, Iteration 0] Starting animation for topic: The Problem of Context Rot in Long-Context LLMs\n",
      "[Topic 0, Iteration 0] Running coding agent...\n",
      "[Topic 0, Iteration 1] Render failed, retrying (iteration 1/5)...\n",
      "\n",
      "=== RENDER FAILED - Iteration 1/5 ===\n",
      "Error message being passed to Gemini:\n",
      "ERROR: TypeError: Mobject.__init__() got an unexpected keyword argument 'font_family'\n",
      "\n",
      "LOCATION: scene.py, line 260\n",
      "\n",
      "CODE AT LINE 260:\n",
      "│   │   self.construct()                                          │\n",
      "\n",
      "FULL TRACEBACK:\n",
      "\n",
      "Animation 0: Write(Text('Part 1/5: Recursive Language Models')):   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "                                                                                                       \n",
      "\n",
      "Animation 1: FadeIn(Text('The Problem of Context Rot in Long-Context LLMs')):   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "                                                                                                                    \n",
      "\n",
      "Animation 3: FadeIn(Rectangle), etc.:   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "                                                                            \n",
      "\n",
      "Animation 5: FadeOut(Text('The Problem of Context Rot in Long-Context LLMs')):   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "Animation 5: FadeOut(Text('The Problem of Context Rot in Long-Context LLMs')):  73%|███████▎  | 11/15 [00:00<00:00, 100.71it/s]\n",
      "                                                                                                                               \n",
      "\n",
      "Animation 6: FadeIn(VGroup of 3 submobjects), etc.:   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "Animation 6: FadeIn(VGroup of 3 submobjects), etc.:  40%|████      | 6/15 [00:00<00:00, 50.48it/s]\n",
      "Animation 6: FadeIn(VGroup of 3 submobjects), etc.:  80%|████████  | 12/15 [00:00<00:00, 51.10it/s]\n",
      "                                                                                                   \n",
      "\n",
      "Animation 7: Transform(Text('Understanding The Problem of Context Rot in Long-Context LLMs')):   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Animation 7: Transform(Text('Understanding The Problem of Context Rot in Long-Context LLMs')): 100%|██████████| 8/8 [00:00<00:00, 73.32it/s]\n",
      "                                                                                                                                            \n",
      "\n",
      "Animation 9: LaggedStart(Group), etc.:   0%|          | 0/60 [00:00<?, ?it/s]\n",
      "Animation 9: LaggedStart(Group), etc.:   3%|▎         | 2/60 [00:01<00:34,  1.67it/s]\n",
      "Animation 9: LaggedStart(Group), etc.:   5%|▌         | 3/60 [00:02<00:54,  1.05it/s]\n",
      "Animation 9: LaggedStart(Group), etc.:   7%|▋         | 4/60 [00:04<01:03,  1.14s/it]\n",
      "Animation 9: LaggedStart(Group), etc.:  13%|█▎        | 8/60 [00:04<00:24,  2.11it/s]\n",
      "Animation 9: LaggedStart(Group), etc.:  17%|█▋        | 10/60 [00:05<00:21,  2.37it/s]\n",
      "Animation 9: LaggedStart(Group), etc.:  38%|███▊      | 23/60 [00:05<00:04,  9.02it/s]\n",
      "Animation 9: LaggedStart(Group), etc.:  58%|█████▊    | 35/60 [00:05<00:01, 16.30it/s]\n",
      "Animation 9: LaggedStart(Group), etc.:  77%|███████▋  | 46/60 [00:05<00:00, 24.56it/s]\n",
      "Animation 9: LaggedStart(Group), etc.:  97%|█████████▋| 58/60 [00:05<00:00, 34.92it/s]\n",
      "                                                                                      \n",
      "\n",
      "Animation 11: Transform(Text('Understanding The Problem of Context Rot in Long-Context LLMs')):   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Animation 11: Transform(Text('Understanding The Problem of Context Rot in Long-Context LLMs')):  75%|███████▌  | 6/8 [00:00<00:00, 55.09it/s]\n",
      "                                                                                                                                             \n",
      "\n",
      "Animation 13: _MethodAnimation(VGroup of 3 submobjects), etc.:   0%|          | 0/23 [00:00<?, ?it/s]\n",
      "Animation 13: _MethodAnimation(VGroup of 3 submobjects), etc.:  17%|█▋        | 4/23 [00:00<00:00, 25.97it/s]\n",
      "Animation 13: _MethodAnimation(VGroup of 3 submobjects), etc.:  35%|███▍      | 8/23 [00:00<00:00, 28.73it/s]\n",
      "Animation 13: _MethodAnimation(VGroup of 3 submobjects), etc.:  52%|█████▏    | 12/23 [00:00<00:00, 29.17it/s]\n",
      "Animation 13: _MethodAnimation(VGroup of 3 submobjects), etc.:  70%|██████▉   | 16/23 [00:00<00:00, 29.87it/s]\n",
      "Animation 13: _MethodAnimation(VGroup of 3 submobjects), etc.:  87%|████████▋ | 20/23 [00:00<00:00, 30.08it/s]\n",
      "                                                                                                              \n",
      "\n",
      "Animation 14: Transform(VGroup of 50 submobjects), etc.:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Animation 14: Transform(VGroup of 50 submobjects), etc.:  30%|███       | 9/30 [00:00<00:00, 80.89it/s]\n",
      "Animation 14: Transform(VGroup of 50 submobjects), etc.:  60%|██████    | 18/30 [00:00<00:00, 82.39it/s]\n",
      "Animation 14: Transform(VGroup of 50 submobjects), etc.:  90%|█████████ | 27/30 [00:00<00:00, 82.26it/s]\n",
      "                                                                                                        \n",
      "\n",
      "Animation 16: FadeOut(VGroup of 3 submobjects), etc.:   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "Animation 16: FadeOut(VGroup of 3 submobjects), etc.:  33%|███▎      | 5/15 [00:00<00:00, 41.92it/s]\n",
      "Animation 16: FadeOut(VGroup of 3 submobjects), etc.:  67%|██████▋   | 10/15 [00:00<00:00, 37.61it/s]\n",
      "Animation 16: FadeOut(VGroup of 3 submobjects), etc.: 100%|██████████| 15/15 [00:00<00:00, 39.91it/s]\n",
      "                                                                                                     \n",
      "\n",
      "Animation 17: Transform(Text('Understanding The Problem of Context Rot in Long-Context LLMs')):   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Animation 17: Transform(Text('Understanding The Problem of Context Rot in Long-Context LLMs')):  88%|████████▊ | 7/8 [00:00<00:00, 66.73it/s]\n",
      "                                                                                                                                             \n",
      "\n",
      "Animation 19: FadeIn(VGroup of 3 submobjects), etc.:   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "Animation 19: FadeIn(VGroup of 3 submobjects), etc.:  67%|██████▋   | 10/15 [00:00<00:00, 98.11it/s]\n",
      "                                                                                                    \n",
      "\n",
      "Animation 21: _MethodAnimation(Circle):   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "                                                                              \n",
      "\n",
      "Animation 22: Indicate(Line), etc.:   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "                                                                          \n",
      "\n",
      "Animation 23: _MethodAnimation(Circle):   0%|          | 0/23 [00:00<?, ?it/s]\n",
      "                                                                              \n",
      "\n",
      "Animation 24: _MethodAnimation(Circle), etc.:   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "                                                                                    \n",
      "\n",
      "Animation 26: FadeOut(VGroup of 3 submobjects), etc.:   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "Animation 26: FadeOut(VGroup of 3 submobjects), etc.:  73%|███████▎  | 11/15 [00:00<00:00, 107.01it/s]\n",
      "                                                                                                      \n",
      "\n",
      "Animation 27: Transform(Text('Understanding The Problem of Context Rot in Long-Context LLMs')):   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "                                                                                                                                     \n",
      "\n",
      "Animation 29: Create(Axes of 2 submobjects), etc.:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Animation 29: Create(Axes of 2 submobjects), etc.:  67%|██████▋   | 20/30 [00:00<00:00, 196.41it/s]\n",
      "                                                                                                   \n",
      "\n",
      "Animation 30: Create(ParametricFunction), etc.:   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "                                                                                      \n",
      "\n",
      "Animation 31: Create(ParametricFunction), etc.:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "                                                                                      \n",
      "\n",
      "Animation 33: FadeOut(VGroup of 8 submobjects):   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "Animation 33: FadeOut(VGroup of 8 submobjects):  80%|████████  | 12/15 [00:00<00:00, 109.37it/s]\n",
      "                                                                                                \n",
      "\n",
      "Animation 34: Transform(Text('Understanding The Problem of Context Rot in Long-Context LLMs')):   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "                                                                                                                                     \n",
      "\n",
      "Animation 36: FadeIn(VGroup of 3 submobjects), etc.:   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "                                                                                           \n",
      "\n",
      "Animation 37: _MethodAnimation(VGroup of 10 submobjects):   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "                                                                                                \n",
      "\n",
      "Animation 38: _MethodAnimation(Square), etc.:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Animation 38: _MethodAnimation(Square), etc.:  43%|████▎     | 13/30 [00:00<00:00, 127.75it/s]\n",
      "Animation 38: _MethodAnimation(Square), etc.:  90%|█████████ | 27/30 [00:00<00:00, 129.83it/s]\n",
      "                                                                                              \n",
      "\n",
      "Animation 39: FadeIn(VGroup of 3 submobjects):   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "                                                                                     \n",
      "\n",
      "Animation 41: FadeOut(VGroup of 3 submobjects), etc.:   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "Animation 41: FadeOut(VGroup of 3 submobjects), etc.: 100%|██████████| 15/15 [00:00<00:00, 137.92it/s]\n",
      "                                                                                                      \n",
      "\n",
      "Animation 42: Transform(Text('Understanding The Problem of Context Rot in Long-Context LLMs')):   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "                                                                                                                                     \n",
      "\n",
      "Animation 44: FadeIn(Square), etc.:   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "                                                                          \n",
      "\n",
      "Animation 45: _MethodAnimation(Rectangle):   0%|          | 0/12 [00:00<?, ?it/s]\n",
      "                                                                                 \n",
      "\n",
      "Animation 46: _MethodAnimation(Rectangle):   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "                                                                                \n",
      "\n",
      "Animation 47: FadeOut(Rectangle):   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "                                                                       \n",
      "\n",
      "Animation 48: _MethodAnimation(Rectangle):   0%|          | 0/12 [00:00<?, ?it/s]\n",
      "                                                                                 \n",
      "\n",
      "Animation 49: _MethodAnimation(Rectangle):   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "                                                                                \n",
      "\n",
      "Animation 50: FadeOut(Rectangle):   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "                                                                       \n",
      "\n",
      "Animation 51: _MethodAnimation(Rectangle):   0%|          | 0/12 [00:00<?, ?it/s]\n",
      "                                                                                 \n",
      "\n",
      "Animation 52: _MethodAnimation(Rectangle):   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "                                                                                \n",
      "\n",
      "Animation 53: FadeOut(Rectangle):   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "                                                                       \n",
      "\n",
      "Animation 54: Write(Text('Out-of-Core Algorithm')):   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "                                                                                          \n",
      "\n",
      "Animation 56: FadeOut(Square), etc.:   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "                                                                           \n",
      "\n",
      "Animation 57: Transform(Text('Understanding The Problem of Context Rot in Long-Context LLMs')):   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "                                                                                                                                     \n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packa │\n",
      "│ ges/manim/cli/render/commands.py:125 in render                               │\n",
      "│                                                                              │\n",
      "│   122 │   │   │   try:                                                       │\n",
      "│   123 │   │   │   │   with tempconfig({}):                                   │\n",
      "│   124 │   │   │   │   │   scene = SceneClass()                               │\n",
      "│ ❱ 125 │   │   │   │   │   scene.render()                                     │\n",
      "│   126 │   │   │   except Exception:                                          │\n",
      "│   127 │   │   │   │   error_console.print_exception()                        │\n",
      "│   128 │   │   │   │   sys.exit(1)                                            │\n",
      "│                                                                              │\n",
      "│ /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packa │\n",
      "│ ges/manim/scene/scene.py:260 in render                                       │\n",
      "│                                                                              │\n",
      "│    257 │   │   \"\"\"                                                           │\n",
      "│    258 │   │   self.setup()                                                  │\n",
      "│    259 │   │   try:                                                          │\n",
      "│ ❱  260 │   │   │   self.construct()                                          │\n",
      "│    261 │   │   except EndSceneEarlyException:                                │\n",
      "│    262 │   │   │   pass                                                      │\n",
      "│    263 │   │   except RerunSceneException:                                   │\n",
      "│                                                                              │\n",
      "│ /Users/milanpatel/Desktop/Knowlify/Agentic                                   │\n",
      "│ Approach/examples/agent_workspace/animation_workspace/scene.py:305 in        │\n",
      "│ construct                                                                    │\n",
      "│                                                                              │\n",
      "│   302 │   │                                                                  │\n",
      "│   303 │   │   interpreter = RoundedRectangle(corner_radius=0.2, width=3, hei │\n",
      "│   304 │   │   int_label = Text(\"Python REPL\", font_size=24, color=GREEN).nex │\n",
      "│ ❱ 305 │   │   code_line = Text(\"read(chapter_1)\", font_family=\"Monospace\", f │\n",
      "│   306 │   │                                                                  │\n",
      "│   307 │   │   self.play(FadeIn(scroll), FadeIn(scroll_lines), FadeIn(env_lab │\n",
      "│   308 │   │   self.play(Create(interpreter), FadeIn(int_label))              │\n",
      "│                                                                              │\n",
      "│ /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packa │\n",
      "│ ges/manim/mobject/text/text_mobject.py:516 in __init__                       │\n",
      "│                                                                              │\n",
      "│    513 │   │   parsed_color: ManimColor = ManimColor(color) if color else VM │\n",
      "│    514 │   │   file_name = self._text2svg(parsed_color.to_hex())             │\n",
      "│    515 │   │   PangoUtils.remove_last_M(file_name)                           │\n",
      "│ ❱  516 │   │   super().__init__(                                             │\n",
      "│    517 │   │   │   file_name,                                                │\n",
      "│    518 │   │   │   fill_opacity=fill_opacity,                                │\n",
      "│    519 │   │   │   stroke_width=stroke_width,                                │\n",
      "│                                                                              │\n",
      "│ /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packa │\n",
      "│ ges/manim/mobject/svg/svg_mobject.py:115 in __init__                         │\n",
      "│                                                                              │\n",
      "│   112 │   │   use_svg_cache: bool = True,                                    │\n",
      "│   113 │   │   **kwargs: Any,                                                 │\n",
      "│   114 │   ):                                                                 │\n",
      "│ ❱ 115 │   │   super().__init__(color=None, stroke_color=None, fill_color=Non │\n",
      "│   116 │   │                                                                  │\n",
      "│   117 │   │   # process keyword arguments                                    │\n",
      "│   118 │   │   self.file_name = Path(file_name) if file_name is not None else │\n",
      "│                                                                              │\n",
      "│ /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packa │\n",
      "│ ges/manim/mobject/types/vectorized_mobject.py:159 in __init__                │\n",
      "│                                                                              │\n",
      "│    156 │   │   │   0, 1, n_points_per_cubic_curve                            │\n",
      "│    157 │   │   )                                                             │\n",
      "│    158 │   │   self.cap_style: CapStyleType = cap_style                      │\n",
      "│ ❱  159 │   │   super().__init__(**kwargs)                                    │\n",
      "│    160 │   │   self.submobjects: list[VMobject]                              │\n",
      "│    161 │   │                                                                 │\n",
      "│    162 │   │   # TODO: Find where color overwrites are happening and remove  │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "TypeError: Mobject.__init__() got an unexpected keyword argument 'font_family'\n",
      "\n",
      "============================================================\n",
      "\n",
      "[Topic 0, Iteration 1] Success! Video saved to /Users/milanpatel/Desktop/Knowlify/Agentic Approach/examples/agent_workspace/rendered_videos/the_problem_of_context_rot_in_long-context_llms_0.mp4\n"
     ]
    }
   ],
   "source": [
    "# Define a progress callback to see what's happening\n",
    "def progress_callback(topic_idx, iteration, message):\n",
    "    print(f\"[Topic {topic_idx}, Iteration {iteration}] {message}\")\n",
    "\n",
    "# Now run with progress tracking\n",
    "storyboard_0_animation_results = eduly_animation_client.animate_single(\n",
    "    breakdown=breakdown_obj,\n",
    "    storyboard=test_storyboard_0,\n",
    "    topic_index=0,\n",
    "    max_iterations=5,\n",
    "    on_progress=progress_callback,  # Add this!\n",
    "    ratelimit=0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
