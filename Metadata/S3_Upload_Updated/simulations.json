{
    "topic": "The mathematical concept being demonstrated is the use of frequency-based priors to estimate the likelihood of",
    "code": [
        "from manim_imports_ext import *\n",
        "from tqdm import tqdm as ProgressDisplay\n",
        "from scipy.stats import entropy\n",
        "\n",
        "\n",
        "MISS = np.uint8(0)\n",
        "MISPLACED = np.uint8(1)\n",
        "EXACT = np.uint8(2)\n",
        "\n",
        "DATA_DIR = os.path.join(\n",
        "    os.path.dirname(os.path.realpath(__file__)),\n",
        "    \"data\",\n",
        ")\n",
        "SHORT_WORD_LIST_FILE = os.path.join(DATA_DIR, \"possible_words.txt\")\n",
        "LONG_WORD_LIST_FILE = os.path.join(DATA_DIR, \"allowed_words.txt\")\n",
        "WORD_FREQ_FILE = os.path.join(DATA_DIR, \"wordle_words_freqs_full.txt\")\n",
        "WORD_FREQ_MAP_FILE = os.path.join(DATA_DIR, \"freq_map.json\")\n",
        "SECOND_GUESS_MAP_FILE = os.path.join(DATA_DIR, \"second_guess_map.json\")\n",
        "PATTERN_MATRIX_FILE = os.path.join(DATA_DIR, \"pattern_matrix.npy\")\n",
        "ENT_SCORE_PAIRS_FILE = os.path.join(DATA_DIR, \"ent_score_pairs.json\")\n",
        "\n",
        "# To store the large grid of patterns at run time\n",
        "PATTERN_GRID_DATA = dict()\n",
        "\n",
        "CHUNK_LENGTH = 13000\n",
        "\n",
        "\n",
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "\n",
        "def safe_log2(x):\n",
        "    return math.log2(x) if x > 0 else 0\n",
        "\n",
        "\n",
        "# Reading from files\n",
        "\n",
        "def get_word_list(short=False):\n",
        "    result = []\n",
        "    file = SHORT_WORD_LIST_FILE if short else LONG_WORD_LIST_FILE\n",
        "    with open(file) as fp:\n",
        "        result.extend([word.strip() for word in fp.readlines()])\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_word_frequencies(regenerate=False):\n",
        "    if os.path.exists(WORD_FREQ_MAP_FILE) or regenerate:\n",
        "        with open(WORD_FREQ_MAP_FILE) as fp:\n",
        "            result = json.load(fp)\n",
        "        return result\n",
        "    # Otherwise, regenerate\n",
        "    freq_map = dict()\n",
        "    with open(WORD_FREQ_FILE) as fp:\n",
        "        for line in fp.readlines():\n",
        "            pieces = line.split(' ')\n",
        "            word = pieces[0]\n",
        "            freqs = [\n",
        "                float(piece.strip())\n",
        "                for piece in pieces[1:]\n",
        "            ]\n",
        "            freq_map[word] = np.mean(freqs[-5:])\n",
        "    with open(WORD_FREQ_MAP_FILE, 'w') as fp:\n",
        "        json.dump(freq_map, fp)\n",
        "    return freq_map\n",
        "\n",
        "\n",
        "def get_frequency_based_priors(n_common=3000, width_under_sigmoid=10):\n",
        "    \"\"\"\n",
        "    We know that that list of wordle answers was curated by some human\n",
        "    based on whether they're sufficiently common. This function aims\n",
        "    to associate each word with the likelihood that it would actually\n",
        "    be selected for the final answer.\n",
        "\n",
        "    Sort the words by frequency, then apply a sigmoid along it.\n",
        "    \"\"\"\n",
        "    freq_map = get_word_frequencies()\n",
        "    words = np.array(list(freq_map.keys()))\n",
        "    freqs = np.array([freq_map[w] for w in words])\n",
        "    arg_sort = freqs.argsort()\n",
        "    sorted_words = words[arg_sort]\n",
        "\n",
        "    # We want to imagine taking this sorted list, and putting it on a number\n",
        "    # line so that it's length is 10, situating it so that the n_common most common\n",
        "    # words are positive, then applying a sigmoid\n",
        "    x_width = width_under_sigmoid\n",
        "    c = x_width * (-0.5 + n_common / len(words))\n",
        "    xs = np.linspace(c - x_width / 2, c + x_width / 2, len(words))\n",
        "    priors = dict()\n",
        "    for word, x in zip(sorted_words, xs):\n",
        "        priors[word] = sigmoid(x)\n",
        "    return priors\n",
        "\n",
        "\n",
        "def get_true_wordle_prior():\n",
        "    words = get_word_list()\n",
        "    short_words = get_word_list(short=True)\n",
        "    return dict(\n",
        "        (w, int(w in short_words))\n",
        "        for w in words\n",
        "    )\n",
        "\n",
        "\n",
        "# Generating color patterns between strings, etc.\n",
        "\n",
        "\n",
        "def words_to_int_arrays(words):\n",
        "    return np.array([[ord(c)for c in w] for w in words], dtype=np.uint8)\n",
        "\n",
        "\n",
        "def generate_pattern_matrix(words1, words2):\n",
        "    \"\"\"\n",
        "    A pattern for two words represents the wordle-similarity\n",
        "    pattern (grey -> 0, yellow -> 1, green -> 2) but as an integer\n",
        "    between 0 and 3^5. Reading this integer in ternary gives the\n",
        "    associated pattern.\n",
        "\n",
        "    This function computes the pairwise patterns between two lists\n",
        "    of words, returning the result as a grid of hash values. Since\n",
        "    this can be time-consuming, many operations that can be are vectorized\n",
        "    (perhaps at the expense of easier readibility), and the the result\n",
        "    is saved to file so that this only needs to be evaluated once, and\n",
        "    all remaining pattern matching is a lookup.\n",
        "    \"\"\"\n",
        "\n",
        "    # Number of letters/words\n",
        "    nl = len(words1[0])\n",
        "    nw1 = len(words1)  # Number of words\n",
        "    nw2 = len(words2)  # Number of words\n",
        "\n",
        "    # Convert word lists to integer arrays\n",
        "    word_arr1, word_arr2 = map(words_to_int_arrays, (words1, words2))\n",
        "\n",
        "    # equality_grid keeps track of all equalities between all pairs\n",
        "    # of letters in words. Specifically, equality_grid[a, b, i, j]\n",
        "    # is true when words[i][a] == words[b][j]\n",
        "    equality_grid = np.zeros((nw1, nw2, nl, nl), dtype=bool)\n",
        "    for i, j in it.product(range(nl), range(nl)):\n",
        "        equality_grid[:, :, i, j] = np.equal.outer(word_arr1[:, i], word_arr2[:, j])\n",
        "\n",
        "    # full_pattern_matrix[a, b] should represent the 5-color pattern\n",
        "    # for guess a and answer b, with 0 -> grey, 1 -> yellow, 2 -> green\n",
        "    full_pattern_matrix = np.zeros((nw1, nw2, nl), dtype=np.uint8)\n",
        "\n",
        "    # Green pass\n",
        "    for i in range(nl):\n",
        "        matches = equality_grid[:, :, i, i].flatten()  # matches[a, b] is true when words[a][i] = words[b][i]\n",
        "        full_pattern_matrix[:, :, i].flat[matches] = EXACT\n",
        "\n",
        "        for k in range(nl):\n",
        "            # If it's a match, mark all elements associated with\n",
        "            # that letter, both from the guess and answer, as covered.\n",
        "            # That way, it won't trigger the yellow pass.\n",
        "            equality_grid[:, :, k, i].flat[matches] = False\n",
        "            equality_grid[:, :, i, k].flat[matches] = False\n",
        "\n",
        "    # Yellow pass\n",
        "    for i, j in it.product(range(nl), range(nl)):\n",
        "        matches = equality_grid[:, :, i, j].flatten()\n",
        "        full_pattern_matrix[:, :, i].flat[matches] = MISPLACED\n",
        "        for k in range(nl):\n",
        "            # Similar to above, we want to mark this letter\n",
        "            # as taken care of, both for answer and guess\n",
        "            equality_grid[:, :, k, j].flat[matches] = False\n",
        "            equality_grid[:, :, i, k].flat[matches] = False\n",
        "\n",
        "    # Rather than representing a color pattern as a lists of integers,\n",
        "    # store it as a single integer, whose ternary representations corresponds\n",
        "    # to that list of integers.\n",
        "    pattern_matrix = np.dot(\n",
        "        full_pattern_matrix,\n",
        "        (3**np.arange(nl)).astype(np.uint8)\n",
        "    )\n",
        "\n",
        "    return pattern_matrix\n",
        "\n",
        "def generate_pattern_matrix_in_blocks(many_words1, many_words2, block_length=CHUNK_LENGTH):\n",
        "    block_matrix = None\n",
        "    for words1 in chunks(many_words1, block_length):\n",
        "        row = None\n",
        "\n",
        "        for words2 in chunks(many_words2, block_length):\n",
        "            block = generate_pattern_matrix(words1, words2)\n",
        "\n",
        "            if row is None:\n",
        "                row = block\n",
        "            else:\n",
        "                row = np.hstack((row, block))\n",
        "\n",
        "        if block_matrix is None:\n",
        "            block_matrix = row\n",
        "        else:\n",
        "            block_matrix = np.vstack((block_matrix, row))\n",
        "\n",
        "    return block_matrix\n",
        "\n",
        "\n",
        "def generate_full_pattern_matrix():\n",
        "    words = get_word_list()\n",
        "    pattern_matrix = generate_pattern_matrix_in_blocks(words, words)\n",
        "    # Save to file\n",
        "    np.save(PATTERN_MATRIX_FILE, pattern_matrix)\n",
        "    return pattern_matrix\n",
        "\n",
        "\n",
        "def get_pattern_matrix(words1, words2):\n",
        "    if not PATTERN_GRID_DATA:\n",
        "        if not os.path.exists(PATTERN_MATRIX_FILE):\n",
        "            log.info(\"\\n\".join([\n",
        "                \"Generating pattern matrix. This takes a minute, but\",\n",
        "                \"the result will be saved to file so that it only\",\n",
        "                \"needs to be computed once.\",\n",
        "            ]))\n",
        "            generate_full_pattern_matrix()\n",
        "        PATTERN_GRID_DATA['grid'] = np.load(PATTERN_MATRIX_FILE)\n",
        "        PATTERN_GRID_DATA['words_to_index'] = dict(zip(\n",
        "            get_word_list(), it.count()\n",
        "        ))\n",
        "\n",
        "    full_grid = PATTERN_GRID_DATA['grid']\n",
        "    words_to_index = PATTERN_GRID_DATA['words_to_index']\n",
        "\n",
        "    indices1 = [words_to_index[w] for w in words1]\n",
        "    indices2 = [words_to_index[w] for w in words2]\n",
        "    return full_grid[np.ix_(indices1, indices2)]\n",
        "\n",
        "\n",
        "def get_pattern(guess, answer):\n",
        "    if PATTERN_GRID_DATA:\n",
        "        saved_words = PATTERN_GRID_DATA['words_to_index']\n",
        "        if guess in saved_words and answer in saved_words:\n",
        "            return get_pattern_matrix([guess], [answer])[0, 0]\n",
        "    return generate_pattern_matrix([guess], [answer])[0, 0]\n",
        "\n",
        "\n",
        "def pattern_from_string(pattern_string):\n",
        "    return sum((3**i) * int(c) for i, c in enumerate(pattern_string))\n",
        "\n",
        "\n",
        "def pattern_to_int_list(pattern):\n",
        "    result = []\n",
        "    curr = pattern\n",
        "    for x in range(5):\n",
        "        result.append(curr % 3)\n",
        "        curr = curr // 3\n",
        "    return result\n",
        "\n",
        "\n",
        "def pattern_to_string(pattern):\n",
        "    d = {MISS: \"\u2b1b\", MISPLACED: \"\ud83d\udfe8\", EXACT: \"\ud83d\udfe9\"}\n",
        "    return \"\".join(d[x] for x in pattern_to_int_list(pattern))\n",
        "\n",
        "\n",
        "def patterns_to_string(patterns):\n",
        "    return \"\\n\".join(map(pattern_to_string, patterns))\n",
        "\n",
        "\n",
        "def get_possible_words(guess, pattern, word_list):\n",
        "    all_patterns = get_pattern_matrix([guess], word_list).flatten()\n",
        "    return list(np.array(word_list)[all_patterns == pattern])\n",
        "\n",
        "\n",
        "def get_word_buckets(guess, possible_words):\n",
        "    buckets = [[] for x in range(3**5)]\n",
        "    hashes = get_pattern_matrix([guess], possible_words).flatten()\n",
        "    for index, word in zip(hashes, possible_words):\n",
        "        buckets[index].append(word)\n",
        "    return buckets\n",
        "\n",
        "\n",
        "# Functions associated with entropy calculation\n",
        "\n",
        "\n",
        "def get_weights(words, priors):\n",
        "    frequencies = np.array([priors[word] for word in words])\n",
        "    total = frequencies.sum()\n",
        "    if total == 0:\n",
        "        return np.zeros(frequencies.shape)\n",
        "    return frequencies / total\n",
        "\n",
        "\n",
        "def get_pattern_distributions(allowed_words, possible_words, weights):\n",
        "    \"\"\"\n",
        "    For each possible guess in allowed_words, this finds the probability\n",
        "    distribution across all of the 3^5 wordle patterns you could see, assuming\n",
        "    the possible answers are in possible_words with associated probabilities\n",
        "    in weights.\n",
        "\n",
        "    It considers the pattern hash grid between the two lists of words, and uses\n",
        "    that to bucket together words from possible_words which would produce\n",
        "    the same pattern, adding together their corresponding probabilities.\n",
        "    \"\"\"\n",
        "    pattern_matrix = get_pattern_matrix(allowed_words, possible_words)\n",
        "\n",
        "    n = len(allowed_words)\n",
        "    distributions = np.zeros((n, 3**5))\n",
        "    n_range = np.arange(n)\n",
        "    for j, prob in enumerate(weights):\n",
        "        distributions[n_range, pattern_matrix[:, j]] += prob\n",
        "    return distributions\n",
        "\n",
        "\n",
        "def entropy_of_distributions(distributions, atol=1e-12):\n",
        "    axis = len(distributions.shape) - 1\n",
        "    return entropy(distributions, base=2, axis=axis)\n",
        "\n",
        "\n",
        "def get_entropies(allowed_words, possible_words, weights):\n",
        "    if weights.sum() == 0:\n",
        "        return np.zeros(len(allowed_words))\n",
        "    distributions = get_pattern_distributions(allowed_words, possible_words, weights)\n",
        "    return entropy_of_distributions(distributions)\n",
        "\n",
        "\n",
        "def max_bucket_size(guess, possible_words, weights):\n",
        "    dist = get_pattern_distributions([guess], possible_words, weights)\n",
        "    return dist.max()\n",
        "\n",
        "\n",
        "def words_to_max_buckets(possible_words, weights):\n",
        "    return dict(\n",
        "        (word, max_bucket_size(word, possible_words, weights))\n",
        "        for word in ProgressDisplay(possible_words)\n",
        "    )\n",
        "\n",
        "    words_and_maxes = list(w2m.items())\n",
        "    words_and_maxes.sort(key=lambda t: t[1])\n",
        "    words_and_maxes[:-20:-1]\n",
        "\n",
        "\n",
        "def get_bucket_sizes(allowed_words, possible_words):\n",
        "    \"\"\"\n",
        "    Returns a (len(allowed_words), 243) shape array reprenting the size of\n",
        "    word buckets associated with each guess in allowed_words\n",
        "    \"\"\"\n",
        "    weights = np.ones(len(possible_words))\n",
        "    return get_pattern_distributions(allowed_words, possible_words, weights)\n",
        "\n",
        "\n",
        "def get_bucket_counts(allowed_words, possible_words):\n",
        "    \"\"\"\n",
        "    Returns the number of separate buckets that each guess in allowed_words\n",
        "    would separate possible_words into\n",
        "    \"\"\"\n",
        "    bucket_sizes = get_bucket_sizes(allowed_words, possible_words)\n",
        "    return (bucket_sizes > 0).sum(1)\n",
        "\n",
        "\n",
        "# Functions to analyze second guesses\n",
        "\n",
        "\n",
        "def get_average_second_step_entropies(first_guesses, allowed_second_guesses, possible_words, priors):\n",
        "    result = []\n",
        "    weights = get_weights(possible_words, priors)\n",
        "    if weights.sum() == 0:\n",
        "        return np.zeros(len(first_guesses))\n",
        "\n",
        "    distributions = get_pattern_distributions(first_guesses, possible_words, weights)\n",
        "    for first_guess, dist in ProgressDisplay(list(zip(first_guesses, distributions)), leave=False, desc=\"Searching 2nd step entropies\"):\n",
        "        word_buckets = get_word_buckets(first_guess, possible_words)\n",
        "        # List of maximum entropies you could achieve in\n",
        "        # the second step for each pattern you might see\n",
        "        # after this setp\n",
        "        ents2 = np.array([\n",
        "            get_entropies(\n",
        "                allowed_words=allowed_second_guesses,\n",
        "                possible_words=bucket,\n",
        "                weights=get_weights(bucket, priors)\n",
        "            ).max()\n",
        "            for bucket in word_buckets\n",
        "        ])\n",
        "        # Multiply each such maximal entropy by the corresponding\n",
        "        # probability of falling into that bucket\n",
        "        result.append(np.dot(ents2, dist))\n",
        "    return np.array(result)\n",
        "\n",
        "\n",
        "# Solvers\n",
        "\n",
        "def get_guess_values_array(allowed_words, possible_words, priors, look_two_ahead=False):\n",
        "    weights = get_weights(possible_words, priors)\n",
        "    ents1 = get_entropies(allowed_words, possible_words, weights)\n",
        "    probs = np.array([\n",
        "        0 if word not in possible_words else weights[possible_words.index(word)]\n",
        "        for word in allowed_words\n",
        "    ])\n",
        "\n",
        "    if look_two_ahead:\n",
        "        # Look two steps out, but restricted to where second guess is\n",
        "        # amoung the remaining possible words\n",
        "        ents2 = np.zeros(ents1.shape)\n",
        "        top_indices = np.argsort(ents1)[-250:]\n",
        "        ents2[top_indices] = get_average_second_step_entropies(\n",
        "            first_guesses=np.array(allowed_words)[top_indices],\n",
        "            allowed_second_guesses=allowed_words,\n",
        "            possible_words=possible_words,\n",
        "            priors=priors\n",
        "        )\n",
        "        return np.array([ents1, ents2, probs])\n",
        "    else:\n",
        "        return np.array([ents1, probs])\n",
        "\n",
        "\n",
        "def entropy_to_expected_score(ent):\n",
        "    \"\"\"\n",
        "    Based on a regression associating entropies with typical scores\n",
        "    from that point forward in simulated games, this function returns\n",
        "    what the expected number of guesses required will be in a game where\n",
        "    there's a given amount of entropy in the remaining possibilities.\n",
        "    \"\"\"\n",
        "    # Assuming you can definitely get it in the next guess,\n",
        "    # this is the expected score\n",
        "    min_score = 2**(-ent) + 2 * (1 - 2**(-ent))\n",
        "\n",
        "    # To account for the likely uncertainty after the next guess,\n",
        "    # and knowing that entropy of 11.5 bits seems to have average\n",
        "    # score of 3.5, we add a line to account\n",
        "    # we add a line which connects (0, 0) to (3.5, 11.5)\n",
        "    return min_score + 1.5 * ent / 11.5\n",
        "\n",
        "\n",
        "def get_expected_scores(allowed_words, possible_words, priors,\n",
        "                        look_two_ahead=False,\n",
        "                        n_top_candidates_for_two_step=25,\n",
        "                        ):\n",
        "    # Currenty entropy of distribution\n",
        "    weights = get_weights(possible_words, priors)\n",
        "    H0 = entropy_of_distributions(weights)\n",
        "    H1s = get_entropies(allowed_words, possible_words, weights)\n",
        "\n",
        "    word_to_weight = dict(zip(possible_words, weights))\n",
        "    probs = np.array([word_to_weight.get(w, 0) for w in allowed_words])\n",
        "    # If this guess is the true answer, score is 1. Otherwise, it's 1 plus\n",
        "    # the expected number of guesses it will take after getting the corresponding\n",
        "    # amount of information.\n",
        "    expected_scores = probs + (1 - probs) * (1 + entropy_to_expected_score(H0 - H1s))\n",
        "\n",
        "    if not look_two_ahead:\n",
        "        return expected_scores\n",
        "\n",
        "    # For the top candidates, refine the score by looking two steps out\n",
        "    # This is currently quite slow, and could be optimized to be faster.\n",
        "    # But why?\n",
        "    sorted_indices = np.argsort(expected_scores)\n",
        "    allowed_second_guesses = get_word_list()\n",
        "    expected_scores += 1  # Push up the rest\n",
        "    for i in ProgressDisplay(sorted_indices[:n_top_candidates_for_two_step], leave=False):\n",
        "        guess = allowed_words[i]\n",
        "        H1 = H1s[i]\n",
        "        dist = get_pattern_distributions([guess], possible_words, weights)[0]\n",
        "        buckets = get_word_buckets(guess, possible_words)\n",
        "        second_guesses = [\n",
        "            optimal_guess(allowed_second_guesses, bucket, priors, look_two_ahead=False)\n",
        "            for bucket in buckets\n",
        "        ]\n",
        "        H2s = [\n",
        "            get_entropies([guess2], bucket, get_weights(bucket, priors))[0]\n",
        "            for guess2, bucket in zip(second_guesses, buckets)\n",
        "        ]\n",
        "\n",
        "        prob = word_to_weight.get(guess, 0)\n",
        "        expected_scores[i] = sum((\n",
        "            # 1 times Probability guess1 is correct\n",
        "            1 * prob,\n",
        "            # 2 times probability guess2 is correct\n",
        "            2 * (1 - prob) * sum(\n",
        "                p * word_to_weight.get(g2, 0)\n",
        "                for p, g2 in zip(dist, second_guesses)\n",
        "            ),\n",
        "            # 2 plus expected score two steps from now\n",
        "            (1 - prob) * (2 + sum(\n",
        "                p * (1 - word_to_weight.get(g2, 0)) * entropy_to_expected_score(H0 - H1 - H2)\n",
        "                for p, g2, H2 in zip(dist, second_guesses, H2s)\n",
        "            ))\n",
        "        ))\n",
        "    return expected_scores\n",
        "\n",
        "\n",
        "def get_score_lower_bounds(allowed_words, possible_words):\n",
        "    \"\"\"\n",
        "    Assuming a uniform distribution on how likely each element\n",
        "    of possible_words is, this gives the a lower boudn on the\n",
        "    possible score for each word in allowed_words\n",
        "    \"\"\"\n",
        "    bucket_counts = get_bucket_counts(allowed_words, possible_words)\n",
        "    N = len(possible_words)\n",
        "    # Probabilities of getting it in 1\n",
        "    p1s = np.array([w in possible_words for w in allowed_words]) / N\n",
        "    # Probabilities of getting it in 2\n",
        "    p2s = bucket_counts / N - p1s\n",
        "    # Otherwise, assume it's gotten in 3 (which is optimistics)\n",
        "    p3s = 1 - bucket_counts / N\n",
        "    return p1s + 2 * p2s + 3 * p3s\n",
        "\n",
        "\n",
        "def optimal_guess(allowed_words, possible_words, priors,\n",
        "                  look_two_ahead=False,\n",
        "                  optimize_for_uniform_distribution=False,\n",
        "                  purely_maximize_information=False,\n",
        "                  ):\n",
        "    if purely_maximize_information:\n",
        "        if len(possible_words) == 1:\n",
        "            return possible_words[0]\n",
        "        weights = get_weights(possible_words, priors)\n",
        "        ents = get_entropies(allowed_words, possible_words, weights)\n",
        "        return allowed_words[np.argmax(ents)]\n",
        "\n",
        "    # Just experimenting here...\n",
        "    if optimize_for_uniform_distribution:\n",
        "        expected_scores = get_score_lower_bounds(\n",
        "            allowed_words, possible_words\n",
        "        )\n",
        "    else:\n",
        "        expected_scores = get_expected_scores(\n",
        "            allowed_words, possible_words, priors,\n",
        "            look_two_ahead=look_two_ahead\n",
        "        )\n",
        "    return allowed_words[np.argmin(expected_scores)]\n",
        "\n",
        "\n",
        "def brute_force_optimal_guess(all_words, possible_words, priors, n_top_picks=10, display_progress=False):\n",
        "    if len(possible_words) == 0:\n",
        "        # Doesn't matter what to return in this case, so just default to first word in list.\n",
        "        return all_words[0]\n",
        "    # For the suggestions with the top expected scores, just\n",
        "    # actually play the game out from this point to see what\n",
        "    # their actual scores are, and minimize.\n",
        "    expected_scores = get_score_lower_bounds(all_words, possible_words)\n",
        "    top_choices = [all_words[i] for i in np.argsort(expected_scores)[:n_top_picks]]\n",
        "    true_average_scores = []\n",
        "    if display_progress:\n",
        "        iterable = ProgressDisplay(\n",
        "            top_choices,\n",
        "            desc=f\"Possibilities: {len(possible_words)}\",\n",
        "            leave=False\n",
        "        )\n",
        "    else:\n",
        "        iterable = top_choices\n",
        "\n",
        "    for next_guess in iterable:\n",
        "        scores = []\n",
        "        for answer in possible_words:\n",
        "            score = 1\n",
        "            possibilities = list(possible_words)\n",
        "            guess = next_guess\n",
        "            while guess != answer:\n",
        "                possibilities = get_possible_words(\n",
        "                    guess, get_pattern(guess, answer),\n",
        "                    possibilities,\n",
        "                )\n",
        "                # Make recursive? If so, we'd want to keep track of\n",
        "                # the next_guess map and pass it down in the recursive\n",
        "                # subcalls\n",
        "                guess = optimal_guess(\n",
        "                    all_words, possibilities, priors,\n",
        "                    optimize_for_uniform_distribution=True\n",
        "                )\n",
        "                score += 1\n",
        "            scores.append(score)\n",
        "        true_average_scores.append(np.mean(scores))\n",
        "    return top_choices[np.argmin(true_average_scores)]\n",
        "\n",
        "\n",
        "# Run simulated wordle games\n",
        "\n",
        "\n",
        "def get_two_step_score_lower_bound(first_guess, allowed_words, possible_words):\n",
        "    \"\"\"\n",
        "    Useful to prove what the minimum possible average score could be\n",
        "    for a given initial guess\n",
        "    \"\"\"\n",
        "    N = len(possible_words)\n",
        "    buckets = get_word_buckets(first_guess, possible_words)\n",
        "    min_score = 0\n",
        "    for bucket in buckets:\n",
        "        if len(bucket) == 0:\n",
        "            continue\n",
        "        lower_bounds = get_score_lower_bounds(allowed_words, bucket)\n",
        "        min_score += (len(bucket) / N) * lower_bounds.min()\n",
        "    p = (1 / len(possible_words)) * (first_guess in possible_words)\n",
        "    return p + (1 - p) * (1 + min_score)\n",
        "\n",
        "\n",
        "def find_top_scorers(n_top_candidates=100, quiet=True, file_ext=\"\", **kwargs):\n",
        "    # Run find_best_two_step_entropy first\n",
        "    file = os.path.join(get_directories()[\"data\"], \"wordle\", \"best_double_entropies.json\")\n",
        "    with open(file) as fp:\n",
        "        double_ents = json.load(fp)\n",
        "\n",
        "    answers = get_word_list(short=True)\n",
        "    priors = get_true_wordle_prior()\n",
        "    guess_to_score = {}\n",
        "    guess_to_dist = {}\n",
        "\n",
        "    for row in ProgressDisplay(double_ents[:n_top_candidates]):\n",
        "        first_guess = row[0]\n",
        "        result, decision_map = simulate_games(\n",
        "            first_guess, priors=priors,\n",
        "            optimize_for_uniform_distribution=True,\n",
        "            quiet=quiet,\n",
        "            **kwargs,\n",
        "        )\n",
        "        average = result[\"average_score\"]\n",
        "        total = int(np.round(average * len(answers)))\n",
        "        guess_to_score[first_guess] = total\n",
        "        guess_to_dist[first_guess] = result[\"score_distribution\"]\n",
        "\n",
        "    top_scorers = sorted(list(guess_to_score.keys()), key=lambda w: guess_to_score[w])\n",
        "    result = [[w, guess_to_score[w], guess_to_dist[w]] for w in top_scorers]\n",
        "\n",
        "    file = os.path.join(\n",
        "        get_directories()[\"data\"], \"wordle\",\n",
        "        \"best_scores\" + file_ext + \".json\",\n",
        "    )\n",
        "    with open(file, 'w') as fp:\n",
        "        json.dump(result, fp)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def find_best_two_step_entropy():\n",
        "    words = get_word_list()\n",
        "    answers = get_word_list(short=True)\n",
        "    priors = get_true_wordle_prior()\n",
        "\n",
        "    ents = get_entropies(words, answers, get_weights(answers, priors))\n",
        "    sorted_indices = np.argsort(ents)\n",
        "    top_candidates = np.array(words)[sorted_indices[:-250:-1]]\n",
        "    top_ents = ents[sorted_indices[:-250:-1]]\n",
        "\n",
        "    ent_file = os.path.join(get_directories()[\"data\"], \"wordle\", \"best_entropies.json\")\n",
        "    with open(ent_file, 'w') as fp:\n",
        "        json.dump([[tc, te] for tc, te in zip(top_candidates, top_ents)], fp)\n",
        "\n",
        "    ents2 = get_average_second_step_entropies(\n",
        "        top_candidates, words, answers, priors,\n",
        "    )\n",
        "\n",
        "    total_ents = top_ents + ents2\n",
        "    sorted_indices2 = np.argsort(total_ents)\n",
        "\n",
        "    double_ents = [\n",
        "        [top_candidates[i], top_ents[i], ents2[i]]\n",
        "        for i in sorted_indices2[::-1]\n",
        "    ]\n",
        "\n",
        "    ent2_file = os.path.join(get_directories()[\"data\"], \"wordle\", \"best_double_entropies.json\")\n",
        "    with open(ent2_file, 'w') as fp:\n",
        "        json.dump(double_ents, fp)\n",
        "\n",
        "    return double_ents\n",
        "\n",
        "\n",
        "def find_smallest_second_guess_buckets(n_top_picks=100):\n",
        "    all_words = get_word_list()\n",
        "    possibilities = get_word_list(short=True)\n",
        "    priors = get_true_wordle_prior()\n",
        "    weights = get_weights(possibilities, priors)\n",
        "\n",
        "    dists = get_pattern_distributions(all_words, possibilities, weights)\n",
        "    sorted_indices = np.argsort((dists**2).sum(1))\n",
        "\n",
        "    top_indices = sorted_indices[:n_top_picks]\n",
        "    top_picks = np.array(all_words)[top_indices]\n",
        "    top_dists = dists[top_indices]\n",
        "    # Figure out the average number of matching words there will\n",
        "    # be after two steps of game play\n",
        "    avg_ts_buckets = []\n",
        "    for first_guess, dist in ProgressDisplay(list(zip(top_picks, top_dists))):\n",
        "        buckets = get_word_buckets(first_guess, possibilities)\n",
        "        avg_ts_bucket = 0\n",
        "        for p, bucket in zip(dist, buckets):\n",
        "            weights = get_weights(bucket, priors)\n",
        "            sub_dists = get_pattern_distributions(all_words, bucket, weights)\n",
        "            min_ts_bucket = len(bucket) * (sub_dists**2).sum(1).min()\n",
        "            avg_ts_bucket += p * min_ts_bucket\n",
        "        avg_ts_buckets.append(avg_ts_bucket)\n",
        "\n",
        "    result = []\n",
        "    for j in np.argsort(avg_ts_buckets):\n",
        "        i = top_indices[j]\n",
        "        result.append((\n",
        "            # Word\n",
        "            all_words[i],\n",
        "            # Average bucket size after first guess\n",
        "            len(possibilities) * (dists[i]**2).sum(),\n",
        "            # Average bucket size after second, with optimal\n",
        "            # play.\n",
        "            avg_ts_buckets[j],\n",
        "        ))\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_optimal_second_guess_map(first_guess, n_top_picks=10, regenerate=False):\n",
        "    with open(SECOND_GUESS_MAP_FILE) as fp:\n",
        "        all_sgms = json.load(fp)\n",
        "\n",
        "    if first_guess in all_sgms and not regenerate:\n",
        "        return all_sgms[first_guess]\n",
        "\n",
        "    log.info(\"\\n\".join([\n",
        "        f\"Generating optimal second guess map for {first_guess}.\",\n",
        "        \"This involves brute forcing many simulations\",\n",
        "        \"so can take a little while.\"\n",
        "    ]))\n",
        "\n",
        "    sgm = [\"\"] * 3**5\n",
        "    all_words = get_word_list()\n",
        "    wordle_answers = get_word_list(short=True)\n",
        "    priors = get_true_wordle_prior()\n",
        "\n",
        "    buckets = get_word_buckets(first_guess, wordle_answers)\n",
        "    for pattern, bucket in ProgressDisplay(list(enumerate(buckets)), leave=False):\n",
        "        sgm[pattern] = brute_force_optimal_guess(\n",
        "            all_words, bucket, priors,\n",
        "            n_top_picks=n_top_picks,\n",
        "            display_progress=True\n",
        "        )\n",
        "\n",
        "    # Save to file\n",
        "    with open(SECOND_GUESS_MAP_FILE) as fp:\n",
        "        all_sgms = json.load(fp)\n",
        "    all_sgms[first_guess] = sgm\n",
        "    with open(SECOND_GUESS_MAP_FILE, 'w') as fp:\n",
        "        json.dump(all_sgms, fp)\n",
        "\n",
        "    return sgm\n",
        "\n",
        "\n",
        "def gather_entropy_to_score_data(first_guess=\"crane\", priors=None):\n",
        "    words = get_word_list()\n",
        "    answers = get_word_list(short=True)\n",
        "    if priors is None:\n",
        "        priors = get_true_wordle_prior()\n",
        "\n",
        "    # List of entropy/score pairs\n",
        "    ent_score_pairs = []\n",
        "\n",
        "    for answer in ProgressDisplay(answers):\n",
        "        score = 1\n",
        "        possibilities = list(filter(lambda w: priors[w] > 0, words))\n",
        "        guess = first_guess\n",
        "        guesses = []\n",
        "        entropies = []\n",
        "        while True:\n",
        "            guesses.append(guess)\n",
        "            weights = get_weights(possibilities, priors)\n",
        "            entropies.append(entropy_of_distributions(weights))\n",
        "            if guess == answer:\n",
        "                break\n",
        "            possibilities = get_possible_words(\n",
        "                guess, get_pattern(guess, answer), possibilities\n",
        "            )\n",
        "            guess = optimal_guess(words, possibilities, priors)\n",
        "            score += 1\n",
        "\n",
        "        for sc, ent in zip(it.count(1), reversed(entropies)):\n",
        "            ent_score_pairs.append((ent, sc))\n",
        "\n",
        "    with open(ENT_SCORE_PAIRS_FILE, 'w') as fp:\n",
        "        json.dump(ent_score_pairs, fp)\n",
        "\n",
        "    return ent_score_pairs\n",
        "\n",
        "\n",
        "def simulate_games(first_guess=None,\n",
        "                   priors=None,\n",
        "                   look_two_ahead=False,\n",
        "                   optimize_for_uniform_distribution=False,\n",
        "                   second_guess_map=None,\n",
        "                   exclude_seen_words=False,\n",
        "                   test_set=None,\n",
        "                   shuffle=False,\n",
        "                   hard_mode=False,\n",
        "                   purely_maximize_information=False,\n",
        "                   brute_force_optimize=False,\n",
        "                   brute_force_depth=10,\n",
        "                   results_file=None,\n",
        "                   next_guess_map_file=None,\n",
        "                   quiet=False,\n",
        "                   ):\n",
        "    all_words = get_word_list(short=False)\n",
        "    short_word_list = get_word_list(short=True)\n",
        "\n",
        "    if first_guess is None:\n",
        "        first_guess = optimal_guess(\n",
        "            all_words, all_words, priors,\n",
        "            **choice_config\n",
        "        )\n",
        "\n",
        "    if priors is None:\n",
        "        priors = get_frequency_based_priors()\n",
        "\n",
        "    if test_set is None:\n",
        "        test_set = short_word_list\n",
        "\n",
        "    if shuffle:\n",
        "        random.shuffle(test_set)\n",
        "\n",
        "    seen = set()\n",
        "\n",
        "    # Function for choosing the next guess, with a dict to cache\n",
        "    # and reuse results that are seen multiple times in the sim\n",
        "    next_guess_map = {}\n",
        "\n",
        "    def get_next_guess(guesses, patterns, possibilities):\n",
        "        phash = \"\".join(\n",
        "            str(g) + \"\".join(map(str, pattern_to_int_list(p)))\n",
        "            for g, p in zip(guesses, patterns)\n",
        "        )\n",
        "        if second_guess_map is not None and len(patterns) == 1:\n",
        "            next_guess_map[phash] = second_guess_map[patterns[0]]\n",
        "        if phash not in next_guess_map:\n",
        "            choices = all_words\n",
        "            if hard_mode:\n",
        "                for guess, pattern in zip(guesses, patterns):\n",
        "                    choices = get_possible_words(guess, pattern, choices)\n",
        "            if brute_force_optimize:\n",
        "                next_guess_map[phash] = brute_force_optimal_guess(\n",
        "                    choices, possibilities, priors,\n",
        "                    n_top_picks=brute_force_depth,\n",
        "                )\n",
        "            else:\n",
        "                next_guess_map[phash] = optimal_guess(\n",
        "                    choices, possibilities, priors,\n",
        "                    look_two_ahead=look_two_ahead,\n",
        "                    purely_maximize_information=purely_maximize_information,\n",
        "                    optimize_for_uniform_distribution=optimize_for_uniform_distribution,\n",
        "                )\n",
        "        return next_guess_map[phash]\n",
        "\n",
        "    # Go through each answer in the test set, play the game,\n",
        "    # and keep track of the stats.\n",
        "    scores = np.zeros(0, dtype=int)\n",
        "    game_results = []\n",
        "    for answer in ProgressDisplay(test_set, leave=False, desc=\" Trying all wordle answers\"):\n",
        "        guesses = []\n",
        "        patterns = []\n",
        "        possibility_counts = []\n",
        "        possibilities = list(filter(lambda w: priors[w] > 0, all_words))\n",
        "\n",
        "        if exclude_seen_words:\n",
        "            possibilities = list(filter(lambda w: w not in seen, possibilities))\n",
        "\n",
        "        score = 1\n",
        "        guess = first_guess\n",
        "        while guess != answer:\n",
        "            pattern = get_pattern(guess, answer)\n",
        "            guesses.append(guess)\n",
        "            patterns.append(pattern)\n",
        "            possibilities = get_possible_words(guess, pattern, possibilities)\n",
        "            possibility_counts.append(len(possibilities))\n",
        "            score += 1\n",
        "            guess = get_next_guess(guesses, patterns, possibilities)\n",
        "\n",
        "        # Accumulate stats\n",
        "        scores = np.append(scores, [score])\n",
        "        score_dist = [\n",
        "            int((scores == i).sum())\n",
        "            for i in range(1, scores.max() + 1)\n",
        "        ]\n",
        "        total_guesses = scores.sum()\n",
        "        average = scores.mean()\n",
        "        seen.add(answer)\n",
        "\n",
        "        game_results.append(dict(\n",
        "            score=int(score),\n",
        "            answer=answer,\n",
        "            guesses=guesses,\n",
        "            patterns=list(map(int, patterns)),\n",
        "            reductions=possibility_counts,\n",
        "        ))\n",
        "        # Print outcome\n",
        "        if not quiet:\n",
        "            message = \"\\n\".join([\n",
        "                \"\",\n",
        "                f\"Score: {score}\",\n",
        "                f\"Answer: {answer}\",\n",
        "                f\"Guesses: {guesses}\",\n",
        "                f\"Reductions: {possibility_counts}\",\n",
        "                *patterns_to_string((*patterns, 3**5 - 1)).split(\"\\n\"),\n",
        "                *\" \" * (6 - len(patterns)),\n",
        "                f\"Distribution: {score_dist}\",\n",
        "                f\"Total guesses: {total_guesses}\",\n",
        "                f\"Average: {average}\",\n",
        "                *\" \" * 2,\n",
        "            ])\n",
        "            if answer is not test_set[0]:\n",
        "                # Move cursor back up to the top of the message\n",
        "                n = len(message.split(\"\\n\")) + 1\n",
        "                print((\"\\033[F\\033[K\") * n)\n",
        "            else:\n",
        "                print(\"\\r\\033[K\\n\")\n",
        "            print(message)\n",
        "\n",
        "    final_result = dict(\n",
        "        score_distribution=score_dist,\n",
        "        total_guesses=int(total_guesses),\n",
        "        average_score=float(scores.mean()),\n",
        "        game_results=game_results,\n",
        "    )\n",
        "\n",
        "    # Save results\n",
        "    for obj, file in [(final_result, results_file), (next_guess_map, next_guess_map_file)]:\n",
        "        if file:\n",
        "            path = os.path.join(DATA_DIR, \"simulation_results\", file)\n",
        "            with open(path, 'w') as fp:\n",
        "                json.dump(obj, fp)\n",
        "\n",
        "    return final_result, next_guess_map\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    first_guess = \"salet\"\n",
        "    results, decision_map = simulate_games(\n",
        "        first_guess=first_guess,\n",
        "        priors=get_true_wordle_prior(),\n",
        "        optimize_for_uniform_distribution=True,\n",
        "        # shuffle=True,\n",
        "        # brute_force_optimize=True,\n",
        "        # hard_mode=True,\n",
        "    )\n"
    ]
}